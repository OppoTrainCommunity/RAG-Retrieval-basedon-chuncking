# ğŸ“„ Resume RAG â€” Multi-Resume Retrieval + Two-Model Comparison (OpenRouter)

This repository contains an end-to-end **Resume-Based RAG (Retrieval-Augmented Generation)** system that supports:

* **Uploading one or multiple PDF resumes**
* **Indexing into ChromaDB**
* **Asking questions across ONE resume or ALL resumes**
* **Comparing answers from two different LLMs via OpenRouter**
* (Optional) **Evaluating retrieval + generation** (prompts + scripts ready)

---

## ğŸš€ Whatâ€™s New (Updates)

âœ… **Multi-Resume Support**

* You can upload multiple CVs and query **All resumes** or **a specific candidate**.

âœ… **Two LLMs Comparison**

* Answers are generated by **LLM1** and **LLM2** so you can compare quality.

âœ… **Cleaner RAG Pipeline**

* Each stored chunk includes metadata (resume_id, name/email if detected, chunk_index, chunking_mode).

âœ… **Improved Output**

* Results include **candidate identification** (name/email when possible) + **evidence snippets** from retrieved context.

---

## ğŸ§  How the RAG System Works (Detailed)

### 1) Upload PDFs

You upload one or multiple resume PDFs from the Streamlit UI.

### 2) Parse PDF â†’ Text

The PDF is converted into raw text using `resume_rag/parse_pdf.py`.

### 3) Chunking (Two Modes)

You choose a chunking strategy:

* **Semantic Chunking**: splits by CV section headers (EDUCATION, EXPERIENCE, PROJECTS, SKILLSâ€¦)
* **Sliding Window Chunking**: fixed window with overlap (e.g., chunk_size=900, overlap=150)

### 4) Embeddings + Storage (Chroma)

Each chunk is embedded using:

* `sentence-transformers/all-MiniLM-L6-v2`

Chunks are stored in a single collection (default: `resumes`) and distinguished using metadata:

* `resume_id` (unique per uploaded PDF)
* `candidate_name` (if extracted)
* `candidate_email` (if extracted)
* `chunking_mode` (semantic/window)
* `chunk_index`

### 5) Retrieval

When you ask a question:

* If scope = **All resumes**, retriever searches across all chunks
* If scope = **One resume**, it filters by `resume_id`

### 6) Generation (Two Models via OpenRouter)

The pipeline builds a chain:
**Prompt â†’ Retrieval â†’ LLM â†’ Output**
and runs it twice:

* LLM1
* LLM2

---

## ğŸ“¦ Directory Structure

```
RAG-Retrieval-basedon-chuncking/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ chroma_store/                 # persisted Chroma DB (auto-created)
â”œâ”€â”€ temp_uploads/                 # uploaded files cache (auto-created)
â”‚
â””â”€â”€ resume_rag/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config.py
    â”œâ”€â”€ parse_pdf.py
    â”œâ”€â”€ chunker.py
    â”œâ”€â”€ vector_store_chain.py
    â”œâ”€â”€ pipeline_chain.py
    â”œâ”€â”€ llm_chain.py
    â”œâ”€â”€ eval_chain.py             # (optional) generation eval prompts + scoring
    â”œâ”€â”€ evaluate_chunking.py      # (optional) retrieval eval script
    â””â”€â”€ assets/                   # sample CVs (optional)
```

---

## âš™ï¸ Setup

### 1) Create environment

```bash
conda create -n mini-rag python=3.11 -y
conda activate mini-rag
pip install -r requirements.txt
```

### 2) Add OpenRouter Key

Create a `.env` file at the project root:

```env
OPENROUTER_API_KEY=sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxx
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
```

> Important: `.env` should be in the **same folder as `app.py`**.

### 3) Run the app

```bash
streamlit run app.py
```

---

## ğŸ§© Chunking Strategies

### 1) Semantic Chunking

Splits based on section headers such as:

* EDUCATION
* EXPERIENCE
* PROJECTS
* SKILLS
* CERTIFICATIONS
* SUMMARY

Best for:

* â€œWhich university did the candidate attend?â€
* â€œWhat is the candidateâ€™s GPA?â€

### 2) Sliding Window Chunking

Fixed-size chunks + overlap (configurable in the UI).

Best for:

* Skills lists scattered across the CV
* Long project descriptions
* Multi-hop questions

---

## âœ… Example Questions You Can Ask

### Across ALL resumes

* `Who has experience with FastAPI? Return names + emails + evidence.`
* `List candidates who know React + Node.js.`
* `Who worked on YOLO / Computer Vision?`

### For one resume (select from dropdown)

* `What are the candidateâ€™s main technical skills?`
* `Summarize projects in 3 bullets.`
* `What internships or trainings are mentioned?`

---

## ğŸ§ª Evaluation

### A) Retrieval evaluation (chunking strategies)

If you want to evaluate retrieval quality between strategies:

```bash
python -m resume_rag.evaluate_chunking
```

Outputs:

* `evaluation_results.csv`
* summary printed in terminal

Metrics (example):

* Precision@k
* Recall@k
* Hit Rate
* MRR
* Latency

### B) Generation evaluation (after retrieval evaluation)

After retrieval is good, evaluate the **final answer quality**:

* faithfulness to context
* completeness
* correctness of extracted contact info
* format consistency (JSON/table)

Recommended evaluation prompts (used by `eval_chain.py`):

* **Faithfulness**: â€œDoes the answer contain claims not supported by context?â€
* **Answer correctness**: â€œIs the answer correct given the context?â€
* **Attribution**: â€œAre key claims backed by quoted evidence?â€
* **Formatting**: â€œIs output following required schema?â€

---

## ğŸ† Notes / Best Practices

* Keep **one collection** for all resumes, and use **metadata filters** for per-candidate queries.
* Prefer **Sliding Window** for broad search across multiple candidates.
* Prefer **Semantic** for structured fields (education, phone, email, GPA).

---

## ğŸ‘©â€ğŸ’» Author

Developed by **Tala Dweikat** as part of **OppoTrain RAG Task**.

ğŸ“§ [tala.nazeeh.dowiekat@gmail.com](mailto:tala.nazeeh.dowiekat@gmail.com)
ğŸ”— GitHub: [https://github.com/taladowiekat](https://github.com/taladowiekat)
