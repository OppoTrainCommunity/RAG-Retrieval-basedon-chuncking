{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f80359b",
   "metadata": {},
   "source": [
    "# ðŸ“„ RAG Resume Analysis System\n",
    "\n",
    "A production-ready Retrieval-Augmented Generation (RAG) system for resume/CV analysis.\n",
    "\n",
    "## Features\n",
    "- **Multiple Embedding Models**: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002 (via OpenRouter)\n",
    "- **Three Chunking Strategies**: Fixed-length, Semantic, and Recursive\n",
    "- **Vector Storage**: ChromaDB with persistent storage and A/B testing support\n",
    "- **Comprehensive Evaluation**: Precision, Recall, F1, MRR, Hit Rate, Latency, Context Relevance\n",
    "- **Streamlit UI**: Two-page interface for configuration and evaluation\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "```bash\n",
    "# 1. Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 2. Add resume PDFs to data/ folder\n",
    "\n",
    "# 3. Run Streamlit app\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "## Folder Structure\n",
    "```\n",
    "V-4/\n",
    "â”œâ”€â”€ main.ipynb          # This notebook (all modules)\n",
    "â”œâ”€â”€ app.py              # Streamlit app (generated)\n",
    "â”œâ”€â”€ requirements.txt    # Python dependencies\n",
    "â”œâ”€â”€ data/               # Resume PDFs + metadata\n",
    "â”‚   â”œâ”€â”€ metadata.csv    # Per-CV canonical fields\n",
    "â”‚   â””â”€â”€ ground_truth.json  # Query relevance annotations\n",
    "â”œâ”€â”€ cache/              # Embedding cache (JSON files)\n",
    "â”œâ”€â”€ outputs/            # Evaluation results (CSV/JSON)\n",
    "â””â”€â”€ chroma_db/          # ChromaDB persistent storage\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efcd64a",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d005cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dependencies...\n",
      "  âœ“ chromadb\n",
      "  âœ“ openai\n",
      "  âœ“ pdfplumber\n",
      "  âœ“ numpy\n",
      "  âœ“ pandas\n",
      "  âš¡ Installing scikit-learn...\n",
      "  âœ“ scikit-learn installed\n",
      "  âœ“ nltk\n",
      "  âœ“ tqdm\n",
      "  âœ“ plotly\n",
      "  âœ“ streamlit\n",
      "\n",
      "âœ… All dependencies ready!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies if needed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip.\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "\n",
    "# Check and install required packages\n",
    "required_packages = [\n",
    "    \"chromadb\", \"openai\", \"pdfplumber\", \"numpy\", \"pandas\", \n",
    "    \"scikit-learn\", \"nltk\", \"tqdm\", \"plotly\", \"streamlit\"\n",
    "]\n",
    "\n",
    "print(\"Checking dependencies...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"  âœ“ {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"  âš¡ Installing {package}...\")\n",
    "        install_package(package)\n",
    "        print(f\"  âœ“ {package} installed\")\n",
    "\n",
    "print(\"\\nâœ… All dependencies ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e042acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Any, Callable\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PDF processing\n",
    "import pdfplumber\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Vector database\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# OpenAI/OpenRouter client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b259a0",
   "metadata": {},
   "source": [
    "## 2. Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0b5df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Configuration loaded:\n",
      "   Data directory: c:\\Users\\Mr Computer\\Desktop\\Folders\\CAP\\Training\\Oppotrain Training\\RAG\\V-4\\data\n",
      "   Cache directory: c:\\Users\\Mr Computer\\Desktop\\Folders\\CAP\\Training\\Oppotrain Training\\RAG\\V-4\\cache\n",
      "   Output directory: c:\\Users\\Mr Computer\\Desktop\\Folders\\CAP\\Training\\Oppotrain Training\\RAG\\V-4\\outputs\n",
      "   ChromaDB directory: c:\\Users\\Mr Computer\\Desktop\\Folders\\CAP\\Training\\Oppotrain Training\\RAG\\V-4\\chroma_db\n",
      "\n",
      "ðŸ¤– Available models: 3\n",
      "ðŸ“Š Chunking strategies: ['fixed', 'semantic', 'recursive']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Modify these settings as needed\n",
    "# ============================================================\n",
    "\n",
    "# Directory paths (relative to notebook location)\n",
    "BASE_DIR = Path(\".\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "CHROMA_DIR = BASE_DIR / \"chroma_db\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [DATA_DIR, CACHE_DIR, OUTPUT_DIR, CHROMA_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Available embedding models (OpenRouter compatible)\n",
    "EMBEDDING_MODELS = [\n",
    "    \"openai/text-embedding-3-small\",\n",
    "    \"openai/text-embedding-3-large\", \n",
    "    \"openai/text-embedding-ada-002\",\n",
    "]\n",
    "\n",
    "# Chunking strategies\n",
    "CHUNKING_STRATEGIES = [\"fixed\", \"semantic\", \"recursive\"]\n",
    "\n",
    "# Default chunking parameters\n",
    "DEFAULT_CHUNK_SIZE = 500\n",
    "DEFAULT_OVERLAP = 50\n",
    "DEFAULT_MIN_CHUNK_SIZE = 100\n",
    "\n",
    "# API settings\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# Retrieval settings\n",
    "DEFAULT_TOP_K = 5\n",
    "\n",
    "# Batch processing settings\n",
    "EMBEDDING_BATCH_SIZE = 10\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 1.0  # seconds\n",
    "\n",
    "print(\"ðŸ“ Configuration loaded:\")\n",
    "print(f\"   Data directory: {DATA_DIR.absolute()}\")\n",
    "print(f\"   Cache directory: {CACHE_DIR.absolute()}\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"   ChromaDB directory: {CHROMA_DIR.absolute()}\")\n",
    "print(f\"\\nðŸ¤– Available models: {len(EMBEDDING_MODELS)}\")\n",
    "print(f\"ðŸ“Š Chunking strategies: {CHUNKING_STRATEGIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4102cf",
   "metadata": {},
   "source": [
    "## 3. Base Classes and Helper Functions\n",
    "\n",
    "### 3.1 PDF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7bb781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Processing functions defined âœ“\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text as a string, empty string if extraction fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text_parts = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text_parts.append(page_text)\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(text_parts)\n",
    "        return full_text.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_pdf_files(directory: Path = DATA_DIR) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Get all PDF files from a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory to search for PDFs\n",
    "        \n",
    "    Returns:\n",
    "        List of PDF file paths\n",
    "    \"\"\"\n",
    "    pdf_files = list(directory.glob(\"*.pdf\"))\n",
    "    print(f\"ðŸ“„ Found {len(pdf_files)} PDF files in {directory}\")\n",
    "    return pdf_files\n",
    "\n",
    "\n",
    "# Test PDF extraction\n",
    "print(\"PDF Processing functions defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae126d0b",
   "metadata": {},
   "source": [
    "### 3.2 Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc10df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking functions defined âœ“\n",
      "  â€¢ fixed_length_chunking(text, chunk_size=500, overlap=50)\n",
      "  â€¢ semantic_chunking(text, target_size=500)\n",
      "  â€¢ recursive_chunking(text, max_chunk_size=500)\n",
      "  â€¢ get_chunks(text, strategy='fixed|semantic|recursive')\n"
     ]
    }
   ],
   "source": [
    "def fixed_length_chunking(\n",
    "    text: str, \n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE, \n",
    "    overlap: int = DEFAULT_OVERLAP\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Split text into fixed-size chunks with overlap.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Number of overlapping characters between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dictionaries with text and metadata\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk_text = text[start:end]\n",
    "        \n",
    "        chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"chunk_index\": chunk_idx,\n",
    "            \"start_char\": start,\n",
    "            \"end_char\": min(end, len(text)),\n",
    "            \"strategy\": \"fixed\"\n",
    "        })\n",
    "        \n",
    "        start += chunk_size - overlap\n",
    "        chunk_idx += 1\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def semantic_chunking(\n",
    "    text: str, \n",
    "    target_size: int = DEFAULT_CHUNK_SIZE,\n",
    "    min_size: int = DEFAULT_MIN_CHUNK_SIZE\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Split text at sentence boundaries, combining sentences until target size.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        target_size: Target chunk size in characters\n",
    "        min_size: Minimum chunk size\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dictionaries with text and metadata\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Split into sentences\n",
    "    try:\n",
    "        sentences = sent_tokenize(text)\n",
    "    except Exception:\n",
    "        # Fallback to simple sentence splitting\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    chunk_idx = 0\n",
    "    current_start = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "        # If adding this sentence exceeds target and we have content, save chunk\n",
    "        if current_length + sentence_length > target_size and current_chunk:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"chunk_index\": chunk_idx,\n",
    "                \"start_char\": current_start,\n",
    "                \"end_char\": current_start + len(chunk_text),\n",
    "                \"strategy\": \"semantic\",\n",
    "                \"sentence_count\": len(current_chunk)\n",
    "            })\n",
    "            current_start += len(chunk_text) + 1\n",
    "            chunk_idx += 1\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_length + 1  # +1 for space\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk:\n",
    "        chunk_text = \" \".join(current_chunk)\n",
    "        chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"chunk_index\": chunk_idx,\n",
    "            \"start_char\": current_start,\n",
    "            \"end_char\": current_start + len(chunk_text),\n",
    "            \"strategy\": \"semantic\",\n",
    "            \"sentence_count\": len(current_chunk)\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def recursive_chunking(\n",
    "    text: str,\n",
    "    max_chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "    min_chunk_size: int = DEFAULT_MIN_CHUNK_SIZE,\n",
    "    level: int = 0,\n",
    "    parent_id: Optional[str] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Hierarchically split text: paragraphs -> sentences -> fixed chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        max_chunk_size: Maximum chunk size\n",
    "        min_chunk_size: Minimum chunk size\n",
    "        level: Current recursion level (0=paragraphs, 1=sentences, 2=fixed)\n",
    "        parent_id: ID of parent chunk for tracking hierarchy\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dictionaries with text and hierarchy metadata\n",
    "    \"\"\"\n",
    "    if not text or len(text) <= max_chunk_size:\n",
    "        if text:\n",
    "            return [{\n",
    "                \"text\": text,\n",
    "                \"chunk_index\": 0,\n",
    "                \"start_char\": 0,\n",
    "                \"end_char\": len(text),\n",
    "                \"strategy\": \"recursive\",\n",
    "                \"level\": level,\n",
    "                \"parent_id\": parent_id\n",
    "            }]\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Level 0: Split by paragraphs\n",
    "    if level == 0:\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "    # Level 1: Split by sentences\n",
    "    elif level == 1:\n",
    "        try:\n",
    "            paragraphs = sent_tokenize(text)\n",
    "        except:\n",
    "            paragraphs = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # Level 2: Fixed split\n",
    "    else:\n",
    "        # Fall back to fixed chunking\n",
    "        fixed_chunks = fixed_length_chunking(text, max_chunk_size, overlap=50)\n",
    "        for i, chunk in enumerate(fixed_chunks):\n",
    "            chunk[\"level\"] = level\n",
    "            chunk[\"parent_id\"] = parent_id\n",
    "            chunk[\"strategy\"] = \"recursive\"\n",
    "        return fixed_chunks\n",
    "    \n",
    "    current_start = 0\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        para_id = f\"{parent_id}_{i}\" if parent_id else str(i)\n",
    "        \n",
    "        if len(para) <= max_chunk_size:\n",
    "            chunks.append({\n",
    "                \"text\": para,\n",
    "                \"chunk_index\": len(chunks),\n",
    "                \"start_char\": current_start,\n",
    "                \"end_char\": current_start + len(para),\n",
    "                \"strategy\": \"recursive\",\n",
    "                \"level\": level,\n",
    "                \"parent_id\": parent_id,\n",
    "                \"chunk_id\": para_id\n",
    "            })\n",
    "        else:\n",
    "            # Recursively split\n",
    "            sub_chunks = recursive_chunking(\n",
    "                para, max_chunk_size, min_chunk_size, \n",
    "                level=level + 1, parent_id=para_id\n",
    "            )\n",
    "            for sub_chunk in sub_chunks:\n",
    "                sub_chunk[\"chunk_index\"] = len(chunks)\n",
    "                sub_chunk[\"start_char\"] += current_start\n",
    "                sub_chunk[\"end_char\"] += current_start\n",
    "                chunks.append(sub_chunk)\n",
    "        \n",
    "        current_start += len(para) + 2  # +2 for paragraph separator\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_chunks(\n",
    "    text: str, \n",
    "    strategy: str = \"fixed\",\n",
    "    **kwargs\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Dispatcher function to get chunks using the specified strategy.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        strategy: One of 'fixed', 'semantic', 'recursive'\n",
    "        **kwargs: Additional arguments passed to the chunking function\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    strategy = strategy.lower()\n",
    "    \n",
    "    if strategy == \"fixed\":\n",
    "        return fixed_length_chunking(text, **kwargs)\n",
    "    elif strategy == \"semantic\":\n",
    "        return semantic_chunking(text, **kwargs)\n",
    "    elif strategy == \"recursive\":\n",
    "        return recursive_chunking(text, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown chunking strategy: {strategy}. Use one of {CHUNKING_STRATEGIES}\")\n",
    "\n",
    "\n",
    "print(\"Chunking functions defined âœ“\")\n",
    "print(f\"  â€¢ fixed_length_chunking(text, chunk_size={DEFAULT_CHUNK_SIZE}, overlap={DEFAULT_OVERLAP})\")\n",
    "print(f\"  â€¢ semantic_chunking(text, target_size={DEFAULT_CHUNK_SIZE})\")\n",
    "print(f\"  â€¢ recursive_chunking(text, max_chunk_size={DEFAULT_CHUNK_SIZE})\")\n",
    "print(f\"  â€¢ get_chunks(text, strategy='fixed|semantic|recursive')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e68be0",
   "metadata": {},
   "source": [
    "### 3.3 Embedding Generation with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fdb539c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache functions defined âœ“\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Embedding Cache Functions\n",
    "# ============================================================\n",
    "\n",
    "def get_cache_key(text: str, model: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique cache key for text + model combination.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to embed\n",
    "        model: The embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        SHA256 hash string as cache key\n",
    "    \"\"\"\n",
    "    combined = f\"{model}:{text}\"\n",
    "    return hashlib.sha256(combined.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def get_cache_path(cache_key: str) -> Path:\n",
    "    \"\"\"Get the file path for a cache entry.\"\"\"\n",
    "    return CACHE_DIR / f\"{cache_key}.json\"\n",
    "\n",
    "\n",
    "def load_from_cache(cache_key: str) -> Optional[List[float]]:\n",
    "    \"\"\"\n",
    "    Load embedding from cache if exists.\n",
    "    \n",
    "    Args:\n",
    "        cache_key: The cache key\n",
    "        \n",
    "    Returns:\n",
    "        Cached embedding or None if not found\n",
    "    \"\"\"\n",
    "    cache_path = get_cache_path(cache_key)\n",
    "    if cache_path.exists():\n",
    "        try:\n",
    "            with open(cache_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                return data.get('embedding')\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_to_cache(cache_key: str, embedding: List[float], model: str) -> None:\n",
    "    \"\"\"\n",
    "    Save embedding to cache.\n",
    "    \n",
    "    Args:\n",
    "        cache_key: The cache key\n",
    "        embedding: The embedding vector\n",
    "        model: The model used (for metadata)\n",
    "    \"\"\"\n",
    "    cache_path = get_cache_path(cache_key)\n",
    "    try:\n",
    "        with open(cache_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'embedding': embedding,\n",
    "                'model': model,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }, f)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to save cache: {e}\")\n",
    "\n",
    "\n",
    "def clear_cache() -> int:\n",
    "    \"\"\"Clear all cached embeddings. Returns number of files deleted.\"\"\"\n",
    "    count = 0\n",
    "    for cache_file in CACHE_DIR.glob(\"*.json\"):\n",
    "        cache_file.unlink()\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "print(\"Cache functions defined âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00dbb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding functions defined âœ“\n",
      "  â€¢ validate_openrouter_key(api_key)\n",
      "  â€¢ get_embedding(text, model, api_key)\n",
      "  â€¢ get_embeddings_batch(texts, model, api_key)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# API Key Validation and Embedding Generation\n",
    "# ============================================================\n",
    "\n",
    "def validate_openrouter_key(api_key: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate OpenRouter API key by making a test request.\n",
    "    \n",
    "    Args:\n",
    "        api_key: The OpenRouter API key\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, message)\n",
    "    \"\"\"\n",
    "    if not api_key or len(api_key) < 10:\n",
    "        return False, \"API key is too short or empty\"\n",
    "    \n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            base_url=OPENROUTER_BASE_URL,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        \n",
    "        # Make a minimal embedding request to validate\n",
    "        response = client.embeddings.create(\n",
    "            model=\"openai/text-embedding-3-small\",\n",
    "            input=\"test\"\n",
    "        )\n",
    "        \n",
    "        if response.data and len(response.data) > 0:\n",
    "            return True, \"API key is valid âœ“\"\n",
    "        else:\n",
    "            return False, \"API returned empty response\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"401\" in error_msg or \"unauthorized\" in error_msg.lower():\n",
    "            return False, \"Invalid API key (401 Unauthorized)\"\n",
    "        elif \"429\" in error_msg:\n",
    "            return True, \"API key valid (rate limited - try again later)\"\n",
    "        else:\n",
    "            return False, f\"API error: {error_msg[:100]}\"\n",
    "\n",
    "\n",
    "def get_embedding(\n",
    "    text: str, \n",
    "    model: str, \n",
    "    api_key: str,\n",
    "    use_cache: bool = True\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Get embedding for text using OpenRouter API.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        model: Embedding model name (e.g., 'openai/text-embedding-3-small')\n",
    "        api_key: OpenRouter API key\n",
    "        use_cache: Whether to use caching\n",
    "        \n",
    "    Returns:\n",
    "        Embedding vector as list of floats\n",
    "    \"\"\"\n",
    "    # Check cache first\n",
    "    if use_cache:\n",
    "        cache_key = get_cache_key(text, model)\n",
    "        cached = load_from_cache(cache_key)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "    \n",
    "    # Make API request with retry\n",
    "    client = OpenAI(\n",
    "        base_url=OPENROUTER_BASE_URL,\n",
    "        api_key=api_key\n",
    "    )\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=model,\n",
    "                input=text\n",
    "            )\n",
    "            \n",
    "            embedding = response.data[0].embedding\n",
    "            \n",
    "            # Save to cache\n",
    "            if use_cache:\n",
    "                save_to_cache(cache_key, embedding, model)\n",
    "            \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                wait_time = RETRY_DELAY * (2 ** attempt)  # Exponential backoff\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Failed to get embedding after {MAX_RETRIES} attempts: {e}\")\n",
    "\n",
    "\n",
    "def get_embeddings_batch(\n",
    "    texts: List[str],\n",
    "    model: str,\n",
    "    api_key: str,\n",
    "    use_cache: bool = True,\n",
    "    show_progress: bool = True\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Get embeddings for multiple texts with batching and progress bar.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "        model: Embedding model name\n",
    "        api_key: OpenRouter API key\n",
    "        use_cache: Whether to use caching\n",
    "        show_progress: Whether to show progress bar\n",
    "        \n",
    "    Returns:\n",
    "        List of embedding vectors\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    iterator = tqdm(texts, desc=\"Generating embeddings\") if show_progress else texts\n",
    "    \n",
    "    for text in iterator:\n",
    "        embedding = get_embedding(text, model, api_key, use_cache)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "print(\"Embedding functions defined âœ“\")\n",
    "print(\"  â€¢ validate_openrouter_key(api_key)\")\n",
    "print(\"  â€¢ get_embedding(text, model, api_key)\")\n",
    "print(\"  â€¢ get_embeddings_batch(texts, model, api_key)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4129d",
   "metadata": {},
   "source": [
    "### 3.4 ChromaDB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "140234c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB functions defined âœ“\n",
      "  â€¢ initialize_chromadb(collection_name)\n",
      "  â€¢ index_documents(chunks, embeddings, source_file, collection, ...)\n",
      "  â€¢ retrieve_similar_chunks(query, collection, embedding_model, api_key, top_k)\n",
      "  â€¢ list_collections(), delete_collection(), get_collection_stats()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ChromaDB Vector Database Operations\n",
    "# ============================================================\n",
    "\n",
    "# Global ChromaDB client (persistent)\n",
    "_chroma_client = None\n",
    "\n",
    "def get_chroma_client() -> chromadb.PersistentClient:\n",
    "    \"\"\"Get or create the ChromaDB persistent client.\"\"\"\n",
    "    global _chroma_client\n",
    "    if _chroma_client is None:\n",
    "        _chroma_client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "    return _chroma_client\n",
    "\n",
    "\n",
    "def initialize_chromadb(collection_name: str) -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Initialize or get a ChromaDB collection.\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection\n",
    "        \n",
    "    Returns:\n",
    "        ChromaDB Collection object\n",
    "    \"\"\"\n",
    "    client = get_chroma_client()\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "\n",
    "def get_collection_name(model: str, strategy: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a collection name from model and strategy for A/B testing.\n",
    "    \n",
    "    Args:\n",
    "        model: Embedding model name\n",
    "        strategy: Chunking strategy\n",
    "        \n",
    "    Returns:\n",
    "        Sanitized collection name\n",
    "    \"\"\"\n",
    "    # Sanitize model name (remove slashes, etc.)\n",
    "    model_clean = model.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "    return f\"{model_clean}_{strategy}\"\n",
    "\n",
    "\n",
    "def index_documents(\n",
    "    chunks: List[Dict[str, Any]],\n",
    "    embeddings: List[List[float]],\n",
    "    source_file: str,\n",
    "    collection: chromadb.Collection,\n",
    "    embedding_model: str,\n",
    "    chunking_strategy: str\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Index document chunks into ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries with text and metadata\n",
    "        embeddings: List of embedding vectors\n",
    "        source_file: Source PDF filename\n",
    "        collection: ChromaDB collection\n",
    "        embedding_model: Name of embedding model used\n",
    "        chunking_strategy: Name of chunking strategy used\n",
    "        \n",
    "    Returns:\n",
    "        Number of chunks indexed\n",
    "    \"\"\"\n",
    "    if not chunks or not embeddings:\n",
    "        return 0\n",
    "    \n",
    "    ids = []\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        chunk_id = f\"{source_file}_{chunking_strategy}_{i}\"\n",
    "        \n",
    "        ids.append(chunk_id)\n",
    "        documents.append(chunk[\"text\"])\n",
    "        \n",
    "        metadata = {\n",
    "            \"source_file\": source_file,\n",
    "            \"chunk_index\": chunk.get(\"chunk_index\", i),\n",
    "            \"start_char\": chunk.get(\"start_char\", 0),\n",
    "            \"end_char\": chunk.get(\"end_char\", len(chunk[\"text\"])),\n",
    "            \"chunking_strategy\": chunking_strategy,\n",
    "            \"embedding_model\": embedding_model,\n",
    "            \"indexed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add any extra metadata from chunk\n",
    "        for key in [\"level\", \"parent_id\", \"sentence_count\"]:\n",
    "            if key in chunk:\n",
    "                metadata[key] = str(chunk[key])\n",
    "        \n",
    "        metadatas.append(metadata)\n",
    "    \n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=documents,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    return len(ids)\n",
    "\n",
    "\n",
    "def retrieve_similar_chunks(\n",
    "    query: str,\n",
    "    collection: chromadb.Collection,\n",
    "    embedding_model: str,\n",
    "    api_key: str,\n",
    "    top_k: int = DEFAULT_TOP_K\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve similar chunks for a query.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        collection: ChromaDB collection\n",
    "        embedding_model: Model to use for query embedding\n",
    "        api_key: OpenRouter API key\n",
    "        top_k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of result dictionaries with text, metadata, and distance\n",
    "    \"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = get_embedding(query, embedding_model, api_key)\n",
    "    \n",
    "    # Query collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    if results and results['ids'] and len(results['ids']) > 0:\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            formatted_results.append({\n",
    "                \"chunk_id\": results['ids'][0][i],\n",
    "                \"text\": results['documents'][0][i] if results['documents'] else \"\",\n",
    "                \"metadata\": results['metadatas'][0][i] if results['metadatas'] else {},\n",
    "                \"distance\": results['distances'][0][i] if results['distances'] else 0,\n",
    "                \"similarity\": 1 - results['distances'][0][i] if results['distances'] else 1\n",
    "            })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "\n",
    "def list_collections() -> List[str]:\n",
    "    \"\"\"List all collections in ChromaDB.\"\"\"\n",
    "    client = get_chroma_client()\n",
    "    collections = client.list_collections()\n",
    "    return [c.name for c in collections]\n",
    "\n",
    "\n",
    "def delete_collection(collection_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Delete a collection from ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of collection to delete\n",
    "        \n",
    "    Returns:\n",
    "        True if deleted successfully\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = get_chroma_client()\n",
    "        client.delete_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to delete collection: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_collection_stats(collection_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get statistics for a collection.\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of collection\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with collection statistics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = get_chroma_client()\n",
    "        collection = client.get_collection(collection_name)\n",
    "        count = collection.count()\n",
    "        \n",
    "        # Get sample metadata\n",
    "        sample = collection.peek(limit=1)\n",
    "        \n",
    "        return {\n",
    "            \"name\": collection_name,\n",
    "            \"count\": count,\n",
    "            \"sample_metadata\": sample.get(\"metadatas\", [{}])[0] if sample else {}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"name\": collection_name, \"error\": str(e)}\n",
    "\n",
    "\n",
    "print(\"ChromaDB functions defined âœ“\")\n",
    "print(\"  â€¢ initialize_chromadb(collection_name)\")\n",
    "print(\"  â€¢ index_documents(chunks, embeddings, source_file, collection, ...)\")\n",
    "print(\"  â€¢ retrieve_similar_chunks(query, collection, embedding_model, api_key, top_k)\")\n",
    "print(\"  â€¢ list_collections(), delete_collection(), get_collection_stats()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac4f8f",
   "metadata": {},
   "source": [
    "### 3.5 Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8967bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics defined âœ“\n",
      "  â€¢ calculate_precision(retrieved, relevant)\n",
      "  â€¢ calculate_recall(retrieved, relevant)\n",
      "  â€¢ calculate_f1_score(precision, recall)\n",
      "  â€¢ calculate_mrr(ranked_results, relevant)\n",
      "  â€¢ calculate_hit_rate(results, relevant)\n",
      "  â€¢ measure_latency(func, *args)\n",
      "  â€¢ calculate_context_relevance(query_embedding, retrieved_embeddings)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Evaluation Metrics\n",
    "# ============================================================\n",
    "\n",
    "def calculate_precision(retrieved: List[str], relevant: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate precision: proportion of retrieved items that are relevant.\n",
    "    \n",
    "    Args:\n",
    "        retrieved: List of retrieved item IDs/texts\n",
    "        relevant: List of relevant item IDs/texts\n",
    "        \n",
    "    Returns:\n",
    "        Precision score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    if not retrieved:\n",
    "        return 0.0\n",
    "    \n",
    "    retrieved_set = set(retrieved)\n",
    "    relevant_set = set(relevant)\n",
    "    \n",
    "    true_positives = len(retrieved_set & relevant_set)\n",
    "    return true_positives / len(retrieved)\n",
    "\n",
    "\n",
    "def calculate_recall(retrieved: List[str], relevant: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate recall: proportion of relevant items that were retrieved.\n",
    "    \n",
    "    Args:\n",
    "        retrieved: List of retrieved item IDs/texts\n",
    "        relevant: List of relevant item IDs/texts\n",
    "        \n",
    "    Returns:\n",
    "        Recall score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    \n",
    "    retrieved_set = set(retrieved)\n",
    "    relevant_set = set(relevant)\n",
    "    \n",
    "    true_positives = len(retrieved_set & relevant_set)\n",
    "    return true_positives / len(relevant)\n",
    "\n",
    "\n",
    "def calculate_f1_score(precision: float, recall: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate F1 score: harmonic mean of precision and recall.\n",
    "    \n",
    "    Args:\n",
    "        precision: Precision score\n",
    "        recall: Recall score\n",
    "        \n",
    "    Returns:\n",
    "        F1 score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def calculate_mrr(ranked_results: List[List[str]], relevant: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank.\n",
    "    \n",
    "    Args:\n",
    "        ranked_results: List of ranked result lists (one per query)\n",
    "        relevant: List of relevant item IDs\n",
    "        \n",
    "    Returns:\n",
    "        MRR score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    if not ranked_results:\n",
    "        return 0.0\n",
    "    \n",
    "    relevant_set = set(relevant)\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for results in ranked_results:\n",
    "        for rank, item in enumerate(results, 1):\n",
    "            if item in relevant_set:\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "\n",
    "def calculate_hit_rate(results: List[Dict[str, Any]], relevant: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate hit rate: proportion of queries with at least one relevant result.\n",
    "    \n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "        relevant: List of relevant item IDs/texts\n",
    "        \n",
    "    Returns:\n",
    "        Hit rate (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    \n",
    "    relevant_set = set(relevant)\n",
    "    \n",
    "    # Check if any retrieved item is relevant\n",
    "    for result in results:\n",
    "        chunk_id = result.get(\"chunk_id\", \"\")\n",
    "        text = result.get(\"text\", \"\")\n",
    "        \n",
    "        if chunk_id in relevant_set or text in relevant_set:\n",
    "            return 1.0\n",
    "        \n",
    "        # Also check if any relevant item is a substring of retrieved text\n",
    "        for rel_item in relevant_set:\n",
    "            if rel_item.lower() in text.lower():\n",
    "                return 1.0\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def measure_latency(func: Callable, *args, **kwargs) -> Tuple[Any, float]:\n",
    "    \"\"\"\n",
    "    Measure execution time of a function.\n",
    "    \n",
    "    Args:\n",
    "        func: Function to measure\n",
    "        *args: Positional arguments for the function\n",
    "        **kwargs: Keyword arguments for the function\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (function result, latency in milliseconds)\n",
    "    \"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    latency_ms = (end_time - start_time) * 1000\n",
    "    return result, latency_ms\n",
    "\n",
    "\n",
    "def calculate_context_relevance(\n",
    "    query_embedding: List[float],\n",
    "    retrieved_embeddings: List[List[float]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate average cosine similarity between query and retrieved chunks.\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: Query embedding vector\n",
    "        retrieved_embeddings: List of retrieved chunk embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Average similarity score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    if not retrieved_embeddings:\n",
    "        return 0.0\n",
    "    \n",
    "    query_arr = np.array(query_embedding).reshape(1, -1)\n",
    "    retrieved_arr = np.array(retrieved_embeddings)\n",
    "    \n",
    "    similarities = cosine_similarity(query_arr, retrieved_arr)[0]\n",
    "    return float(np.mean(similarities))\n",
    "\n",
    "\n",
    "print(\"Evaluation metrics defined âœ“\")\n",
    "print(\"  â€¢ calculate_precision(retrieved, relevant)\")\n",
    "print(\"  â€¢ calculate_recall(retrieved, relevant)\")\n",
    "print(\"  â€¢ calculate_f1_score(precision, recall)\")\n",
    "print(\"  â€¢ calculate_mrr(ranked_results, relevant)\")\n",
    "print(\"  â€¢ calculate_hit_rate(results, relevant)\")\n",
    "print(\"  â€¢ measure_latency(func, *args)\")\n",
    "print(\"  â€¢ calculate_context_relevance(query_embedding, retrieved_embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abde5c5",
   "metadata": {},
   "source": [
    "### 3.6 Ground Truth Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ed22c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth functions defined âœ“\n",
      "  â€¢ load_ground_truth(filepath)\n",
      "  â€¢ load_metadata(filepath)\n",
      "  â€¢ map_canonical_to_chunks(canonical_answers, chunks)\n",
      "  â€¢ get_relevant_chunks_for_query(query_id, ground_truth, all_chunks)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Ground Truth and Metadata Management\n",
    "# ============================================================\n",
    "\n",
    "def load_ground_truth(filepath: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load ground truth annotations from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to ground truth JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of query -> relevant items mapping\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = DATA_DIR / \"ground_truth.json\"\n",
    "    else:\n",
    "        filepath = Path(filepath)\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"âš ï¸ Ground truth file not found: {filepath}\")\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error loading ground truth: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def load_metadata(filepath: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load resume metadata from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to metadata CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with resume metadata\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = DATA_DIR / \"metadata.csv\"\n",
    "    else:\n",
    "        filepath = Path(filepath)\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"âš ï¸ Metadata file not found: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        return pd.read_csv(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error loading metadata: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def map_canonical_to_chunks(\n",
    "    canonical_answers: List[str],\n",
    "    chunks: List[Dict[str, Any]],\n",
    "    fuzzy_threshold: float = 0.8\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Map canonical answers to chunk IDs using exact and fuzzy matching.\n",
    "    \n",
    "    Args:\n",
    "        canonical_answers: List of canonical answer strings\n",
    "        chunks: List of chunk dictionaries\n",
    "        fuzzy_threshold: Minimum similarity for fuzzy match (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        List of matching chunk IDs\n",
    "    \"\"\"\n",
    "    matching_chunk_ids = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.get(\"text\", \"\").lower()\n",
    "        chunk_id = chunk.get(\"chunk_id\", str(chunk.get(\"chunk_index\", \"\")))\n",
    "        \n",
    "        for answer in canonical_answers:\n",
    "            answer_lower = answer.lower()\n",
    "            \n",
    "            # Exact substring match\n",
    "            if answer_lower in chunk_text:\n",
    "                matching_chunk_ids.append(chunk_id)\n",
    "                break\n",
    "            \n",
    "            # Simple fuzzy match: check if most words from answer are in chunk\n",
    "            answer_words = set(answer_lower.split())\n",
    "            chunk_words = set(chunk_text.split())\n",
    "            \n",
    "            if answer_words and len(answer_words & chunk_words) / len(answer_words) >= fuzzy_threshold:\n",
    "                matching_chunk_ids.append(chunk_id)\n",
    "                break\n",
    "    \n",
    "    return matching_chunk_ids\n",
    "\n",
    "\n",
    "def get_relevant_chunks_for_query(\n",
    "    query_id: str,\n",
    "    ground_truth: Dict[str, Any],\n",
    "    all_chunks: List[Dict[str, Any]]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get relevant chunk IDs for a query based on ground truth.\n",
    "    \n",
    "    Args:\n",
    "        query_id: The query identifier\n",
    "        ground_truth: Ground truth dictionary\n",
    "        all_chunks: All indexed chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of relevant chunk IDs\n",
    "    \"\"\"\n",
    "    if query_id not in ground_truth:\n",
    "        return []\n",
    "    \n",
    "    query_gt = ground_truth[query_id]\n",
    "    relevant = query_gt.get(\"relevant\", [])\n",
    "    \n",
    "    # Extract canonical values from ground truth\n",
    "    canonical_values = []\n",
    "    for item in relevant:\n",
    "        if isinstance(item, dict):\n",
    "            value = item.get(\"value\", \"\")\n",
    "            if value and item.get(\"type\") != \"not_relevant\":\n",
    "                canonical_values.append(value)\n",
    "        elif isinstance(item, str):\n",
    "            canonical_values.append(item)\n",
    "    \n",
    "    if canonical_values:\n",
    "        return map_canonical_to_chunks(canonical_values, all_chunks)\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "print(\"Ground truth functions defined âœ“\")\n",
    "print(\"  â€¢ load_ground_truth(filepath)\")\n",
    "print(\"  â€¢ load_metadata(filepath)\")\n",
    "print(\"  â€¢ map_canonical_to_chunks(canonical_answers, chunks)\")\n",
    "print(\"  â€¢ get_relevant_chunks_for_query(query_id, ground_truth, all_chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fe11b",
   "metadata": {},
   "source": [
    "## 4. Main Pipelines\n",
    "\n",
    "### 4.1 Resume Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62928a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline defined âœ“\n",
      "  â€¢ process_resumes(pdf_files, chunking_strategy, embedding_model, api_key, ...)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Main Processing Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def process_resumes(\n",
    "    pdf_files: List[Path],\n",
    "    chunking_strategy: str,\n",
    "    embedding_model: str,\n",
    "    api_key: str,\n",
    "    collection_name: str = None,\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "    overlap: int = DEFAULT_OVERLAP,\n",
    "    show_progress: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process resume PDFs: extract text, chunk, embed, and index.\n",
    "    \n",
    "    Args:\n",
    "        pdf_files: List of PDF file paths\n",
    "        chunking_strategy: One of 'fixed', 'semantic', 'recursive'\n",
    "        embedding_model: Embedding model name\n",
    "        api_key: OpenRouter API key\n",
    "        collection_name: ChromaDB collection name (auto-generated if None)\n",
    "        chunk_size: Size for chunking\n",
    "        overlap: Overlap for fixed chunking\n",
    "        show_progress: Show progress bars\n",
    "        \n",
    "    Returns:\n",
    "        Summary dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    if not pdf_files:\n",
    "        return {\"error\": \"No PDF files provided\", \"processed\": 0}\n",
    "    \n",
    "    # Generate collection name if not provided\n",
    "    if collection_name is None:\n",
    "        collection_name = get_collection_name(embedding_model, chunking_strategy)\n",
    "    \n",
    "    # Initialize collection\n",
    "    collection = initialize_chromadb(collection_name)\n",
    "    \n",
    "    # Processing stats\n",
    "    stats = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"embedding_model\": embedding_model,\n",
    "        \"chunking_strategy\": chunking_strategy,\n",
    "        \"files_processed\": 0,\n",
    "        \"files_failed\": 0,\n",
    "        \"total_chunks\": 0,\n",
    "        \"total_characters\": 0,\n",
    "        \"processing_time_ms\": 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Process each PDF\n",
    "    iterator = tqdm(pdf_files, desc=\"Processing PDFs\") if show_progress else pdf_files\n",
    "    \n",
    "    for pdf_path in iterator:\n",
    "        try:\n",
    "            # Extract text\n",
    "            text = extract_text_from_pdf(str(pdf_path))\n",
    "            \n",
    "            if not text:\n",
    "                print(f\"âš ï¸ No text extracted from {pdf_path.name}\")\n",
    "                stats[\"files_failed\"] += 1\n",
    "                continue\n",
    "            \n",
    "            # Chunk text\n",
    "            if chunking_strategy == \"fixed\":\n",
    "                chunks = fixed_length_chunking(text, chunk_size, overlap)\n",
    "            elif chunking_strategy == \"semantic\":\n",
    "                chunks = semantic_chunking(text, chunk_size)\n",
    "            elif chunking_strategy == \"recursive\":\n",
    "                chunks = recursive_chunking(text, chunk_size)\n",
    "            else:\n",
    "                chunks = fixed_length_chunking(text, chunk_size, overlap)\n",
    "            \n",
    "            if not chunks:\n",
    "                print(f\"âš ï¸ No chunks created from {pdf_path.name}\")\n",
    "                stats[\"files_failed\"] += 1\n",
    "                continue\n",
    "            \n",
    "            # Get embeddings\n",
    "            chunk_texts = [c[\"text\"] for c in chunks]\n",
    "            embeddings = get_embeddings_batch(\n",
    "                chunk_texts, embedding_model, api_key, \n",
    "                show_progress=False\n",
    "            )\n",
    "            \n",
    "            # Index documents\n",
    "            indexed_count = index_documents(\n",
    "                chunks, embeddings, pdf_path.name,\n",
    "                collection, embedding_model, chunking_strategy\n",
    "            )\n",
    "            \n",
    "            stats[\"files_processed\"] += 1\n",
    "            stats[\"total_chunks\"] += indexed_count\n",
    "            stats[\"total_characters\"] += len(text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {pdf_path.name}: {e}\")\n",
    "            stats[\"files_failed\"] += 1\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    stats[\"processing_time_ms\"] = (end_time - start_time) * 1000\n",
    "    \n",
    "    print(f\"\\nâœ… Processing complete!\")\n",
    "    print(f\"   Files processed: {stats['files_processed']}\")\n",
    "    print(f\"   Total chunks indexed: {stats['total_chunks']}\")\n",
    "    print(f\"   Collection: {collection_name}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "print(\"Processing pipeline defined âœ“\")\n",
    "print(\"  â€¢ process_resumes(pdf_files, chunking_strategy, embedding_model, api_key, ...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92422205",
   "metadata": {},
   "source": [
    "### 4.2 Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec376d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation pipeline defined âœ“\n",
      "  â€¢ evaluate_retrieval(queries, collection_name, embedding_model, api_key, ...)\n",
      "  â€¢ compare_configurations(queries, configurations, api_key, ...)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Evaluation Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    queries: List[str],\n",
    "    collection_name: str,\n",
    "    embedding_model: str,\n",
    "    api_key: str,\n",
    "    ground_truth: Dict[str, Any] = None,\n",
    "    top_k: int = DEFAULT_TOP_K,\n",
    "    show_progress: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance across queries.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of query strings\n",
    "        collection_name: ChromaDB collection name\n",
    "        embedding_model: Model for query embedding\n",
    "        api_key: OpenRouter API key\n",
    "        ground_truth: Ground truth dictionary (optional)\n",
    "        top_k: Number of results per query\n",
    "        show_progress: Show progress bars\n",
    "        \n",
    "    Returns:\n",
    "        Comprehensive evaluation results dictionary\n",
    "    \"\"\"\n",
    "    if not queries:\n",
    "        return {\"error\": \"No queries provided\"}\n",
    "    \n",
    "    # Initialize collection\n",
    "    collection = initialize_chromadb(collection_name)\n",
    "    \n",
    "    # Results storage\n",
    "    all_results = []\n",
    "    per_query_metrics = []\n",
    "    total_latency_ms = 0\n",
    "    \n",
    "    iterator = tqdm(queries, desc=\"Evaluating queries\") if show_progress else queries\n",
    "    \n",
    "    for i, query in enumerate(iterator):\n",
    "        query_id = f\"q_{i}\"\n",
    "        \n",
    "        # Retrieve with timing\n",
    "        results, latency_ms = measure_latency(\n",
    "            retrieve_similar_chunks,\n",
    "            query, collection, embedding_model, api_key, top_k\n",
    "        )\n",
    "        \n",
    "        total_latency_ms += latency_ms\n",
    "        \n",
    "        # Get retrieved chunk IDs\n",
    "        retrieved_ids = [r.get(\"chunk_id\", \"\") for r in results]\n",
    "        retrieved_texts = [r.get(\"text\", \"\") for r in results]\n",
    "        \n",
    "        # Get relevant items from ground truth if available\n",
    "        relevant_ids = []\n",
    "        if ground_truth and query_id in ground_truth:\n",
    "            gt_entry = ground_truth[query_id]\n",
    "            relevant = gt_entry.get(\"relevant\", [])\n",
    "            for item in relevant:\n",
    "                if isinstance(item, dict) and item.get(\"type\") != \"not_relevant\":\n",
    "                    relevant_ids.append(item.get(\"value\", \"\"))\n",
    "                elif isinstance(item, str):\n",
    "                    relevant_ids.append(item)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = calculate_precision(retrieved_ids, relevant_ids) if relevant_ids else 0\n",
    "        recall = calculate_recall(retrieved_ids, relevant_ids) if relevant_ids else 0\n",
    "        f1 = calculate_f1_score(precision, recall)\n",
    "        hit_rate = calculate_hit_rate(results, relevant_ids) if relevant_ids else 0\n",
    "        \n",
    "        # Context relevance (average similarity of top results)\n",
    "        avg_similarity = np.mean([r.get(\"similarity\", 0) for r in results]) if results else 0\n",
    "        \n",
    "        query_result = {\n",
    "            \"query_id\": query_id,\n",
    "            \"query\": query,\n",
    "            \"retrieved_count\": len(results),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"context_relevance\": avg_similarity,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"top_results\": results[:3]  # Store top 3 for display\n",
    "        }\n",
    "        \n",
    "        per_query_metrics.append(query_result)\n",
    "        all_results.append({\n",
    "            \"query\": query,\n",
    "            \"results\": results\n",
    "        })\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    n_queries = len(queries)\n",
    "    \n",
    "    aggregated = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"embedding_model\": embedding_model,\n",
    "        \"top_k\": top_k,\n",
    "        \"num_queries\": n_queries,\n",
    "        \"avg_precision\": np.mean([m[\"precision\"] for m in per_query_metrics]),\n",
    "        \"avg_recall\": np.mean([m[\"recall\"] for m in per_query_metrics]),\n",
    "        \"avg_f1\": np.mean([m[\"f1\"] for m in per_query_metrics]),\n",
    "        \"avg_hit_rate\": np.mean([m[\"hit_rate\"] for m in per_query_metrics]),\n",
    "        \"avg_context_relevance\": np.mean([m[\"context_relevance\"] for m in per_query_metrics]),\n",
    "        \"avg_latency_ms\": total_latency_ms / n_queries if n_queries > 0 else 0,\n",
    "        \"total_latency_ms\": total_latency_ms,\n",
    "        \"per_query_metrics\": per_query_metrics,\n",
    "        \"all_results\": all_results\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Evaluation Results:\")\n",
    "    print(f\"   Queries evaluated: {n_queries}\")\n",
    "    print(f\"   Avg Precision: {aggregated['avg_precision']:.3f}\")\n",
    "    print(f\"   Avg Recall: {aggregated['avg_recall']:.3f}\")\n",
    "    print(f\"   Avg F1: {aggregated['avg_f1']:.3f}\")\n",
    "    print(f\"   Avg Latency: {aggregated['avg_latency_ms']:.2f} ms\")\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def compare_configurations(\n",
    "    queries: List[str],\n",
    "    configurations: List[Dict[str, str]],\n",
    "    api_key: str,\n",
    "    ground_truth: Dict[str, Any] = None,\n",
    "    top_k: int = DEFAULT_TOP_K\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare multiple model+strategy configurations.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of test queries\n",
    "        configurations: List of dicts with 'model' and 'strategy' keys\n",
    "        api_key: OpenRouter API key\n",
    "        ground_truth: Ground truth dictionary\n",
    "        top_k: Number of results per query\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame comparing all configurations\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for config in configurations:\n",
    "        model = config[\"model\"]\n",
    "        strategy = config[\"strategy\"]\n",
    "        collection_name = get_collection_name(model, strategy)\n",
    "        \n",
    "        print(f\"\\nðŸ” Evaluating: {model} + {strategy}\")\n",
    "        \n",
    "        try:\n",
    "            eval_result = evaluate_retrieval(\n",
    "                queries, collection_name, model, api_key,\n",
    "                ground_truth, top_k, show_progress=False\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                \"Model\": model,\n",
    "                \"Strategy\": strategy,\n",
    "                \"Precision\": eval_result[\"avg_precision\"],\n",
    "                \"Recall\": eval_result[\"avg_recall\"],\n",
    "                \"F1\": eval_result[\"avg_f1\"],\n",
    "                \"Hit Rate\": eval_result[\"avg_hit_rate\"],\n",
    "                \"Context Relevance\": eval_result[\"avg_context_relevance\"],\n",
    "                \"Latency (ms)\": eval_result[\"avg_latency_ms\"]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Error: {e}\")\n",
    "            results.append({\n",
    "                \"Model\": model,\n",
    "                \"Strategy\": strategy,\n",
    "                \"Precision\": 0,\n",
    "                \"Recall\": 0,\n",
    "                \"F1\": 0,\n",
    "                \"Hit Rate\": 0,\n",
    "                \"Context Relevance\": 0,\n",
    "                \"Latency (ms)\": 0,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(\"Evaluation pipeline defined âœ“\")\n",
    "print(\"  â€¢ evaluate_retrieval(queries, collection_name, embedding_model, api_key, ...)\")\n",
    "print(\"  â€¢ compare_configurations(queries, configurations, api_key, ...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13c526",
   "metadata": {},
   "source": [
    "### 4.3 Results Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f4b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export functions defined âœ“\n",
      "  â€¢ export_results_csv(results, filename)\n",
      "  â€¢ export_results_json(results, filename)\n",
      "  â€¢ export_comparison_report(comparison_df, filename)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Results Export Functions\n",
    "# ============================================================\n",
    "\n",
    "def export_results_csv(results: Dict[str, Any], filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Export evaluation results to CSV.\n",
    "    \n",
    "    Args:\n",
    "        results: Evaluation results dictionary\n",
    "        filename: Output filename (auto-generated if None)\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"evaluation_results_{timestamp}.csv\"\n",
    "    \n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    \n",
    "    # Convert per-query metrics to DataFrame\n",
    "    if \"per_query_metrics\" in results:\n",
    "        df = pd.DataFrame(results[\"per_query_metrics\"])\n",
    "        # Remove complex nested columns\n",
    "        columns_to_keep = [c for c in df.columns if c != \"top_results\"]\n",
    "        df = df[columns_to_keep]\n",
    "        df.to_csv(filepath, index=False)\n",
    "    else:\n",
    "        # Export aggregated metrics\n",
    "        df = pd.DataFrame([{\n",
    "            k: v for k, v in results.items() \n",
    "            if not isinstance(v, (list, dict))\n",
    "        }])\n",
    "        df.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(f\"ðŸ“ Results exported to: {filepath}\")\n",
    "    return str(filepath)\n",
    "\n",
    "\n",
    "def export_results_json(results: Dict[str, Any], filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Export evaluation results to JSON.\n",
    "    \n",
    "    Args:\n",
    "        results: Evaluation results dictionary\n",
    "        filename: Output filename (auto-generated if None)\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"evaluation_results_{timestamp}.json\"\n",
    "    \n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    \n",
    "    # Make results JSON serializable\n",
    "    serializable = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, (np.floating, np.integer)):\n",
    "            serializable[key] = float(value)\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            serializable[key] = value.tolist()\n",
    "        else:\n",
    "            serializable[key] = value\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"ðŸ“ Results exported to: {filepath}\")\n",
    "    return str(filepath)\n",
    "\n",
    "\n",
    "def export_comparison_report(\n",
    "    comparison_df: pd.DataFrame,\n",
    "    filename: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Export comparison DataFrame to CSV with summary.\n",
    "    \n",
    "    Args:\n",
    "        comparison_df: DataFrame from compare_configurations\n",
    "        filename: Output filename\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"comparison_report_{timestamp}.csv\"\n",
    "    \n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    comparison_df.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(f\"ðŸ“ Comparison report exported to: {filepath}\")\n",
    "    return str(filepath)\n",
    "\n",
    "\n",
    "print(\"Export functions defined âœ“\")\n",
    "print(\"  â€¢ export_results_csv(results, filename)\")\n",
    "print(\"  â€¢ export_results_json(results, filename)\")\n",
    "print(\"  â€¢ export_comparison_report(comparison_df, filename)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207090ea",
   "metadata": {},
   "source": [
    "## 5. Streamlit Application\n",
    "\n",
    "Run the cell below to generate `app.py`, then launch with:\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d4b643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\"\"\"\n",
    "RAG Resume Analysis - Streamlit Application\n",
    "Two-page UI for configuration/upload and evaluation/testing\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Try to import NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt_tab')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "    NLTK_AVAILABLE = True\n",
    "except:\n",
    "    NLTK_AVAILABLE = False\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "BASE_DIR = Path(\".\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "CHROMA_DIR = BASE_DIR / \"chroma_db\"\n",
    "\n",
    "for dir_path in [DATA_DIR, CACHE_DIR, OUTPUT_DIR, CHROMA_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "EMBEDDING_MODELS = [\n",
    "    \"openai/text-embedding-3-small\",\n",
    "    \"openai/text-embedding-3-large\",\n",
    "    \"openai/text-embedding-ada-002\",\n",
    "]\n",
    "\n",
    "CHUNKING_STRATEGIES = [\"fixed\", \"semantic\", \"recursive\"]\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "DEFAULT_CHUNK_SIZE = 500\n",
    "DEFAULT_OVERLAP = 50\n",
    "DEFAULT_TOP_K = 5\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 1.0\n",
    "\n",
    "# ============================================================\n",
    "# Helper Functions (copied from notebook for standalone app)\n",
    "# ============================================================\n",
    "\n",
    "def get_cache_key(text: str, model: str) -> str:\n",
    "    combined = f\"{model}:{text}\"\n",
    "    return hashlib.sha256(combined.encode()).hexdigest()\n",
    "\n",
    "def get_cache_path(cache_key: str) -> Path:\n",
    "    return CACHE_DIR / f\"{cache_key}.json\"\n",
    "\n",
    "def load_from_cache(cache_key: str) -> Optional[List[float]]:\n",
    "    cache_path = get_cache_path(cache_key)\n",
    "    if cache_path.exists():\n",
    "        try:\n",
    "            with open(cache_path, 'r') as f:\n",
    "                return json.load(f).get('embedding')\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def save_to_cache(cache_key: str, embedding: List[float], model: str):\n",
    "    try:\n",
    "        with open(get_cache_path(cache_key), 'w') as f:\n",
    "            json.dump({'embedding': embedding, 'model': model, 'timestamp': datetime.now().isoformat()}, f)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def validate_openrouter_key(api_key: str) -> Tuple[bool, str]:\n",
    "    if not api_key or len(api_key) < 10:\n",
    "        return False, \"API key is too short or empty\"\n",
    "    try:\n",
    "        client = OpenAI(base_url=OPENROUTER_BASE_URL, api_key=api_key)\n",
    "        response = client.embeddings.create(model=\"openai/text-embedding-3-small\", input=\"test\")\n",
    "        if response.data:\n",
    "            return True, \"API key is valid âœ“\"\n",
    "        return False, \"API returned empty response\"\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"401\" in error_msg:\n",
    "            return False, \"Invalid API key\"\n",
    "        elif \"429\" in error_msg:\n",
    "            return True, \"API key valid (rate limited)\"\n",
    "        return False, f\"Error: {error_msg[:100]}\"\n",
    "\n",
    "def get_embedding(text: str, model: str, api_key: str, use_cache: bool = True) -> List[float]:\n",
    "    if use_cache:\n",
    "        cache_key = get_cache_key(text, model)\n",
    "        cached = load_from_cache(cache_key)\n",
    "        if cached:\n",
    "            return cached\n",
    "    \n",
    "    client = OpenAI(base_url=OPENROUTER_BASE_URL, api_key=api_key)\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.embeddings.create(model=model, input=text)\n",
    "            embedding = response.data[0].embedding\n",
    "            if use_cache:\n",
    "                save_to_cache(cache_key, embedding, model)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY * (2 ** attempt))\n",
    "            else:\n",
    "                raise RuntimeError(f\"Failed after {MAX_RETRIES} attempts: {e}\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    try:\n",
    "        text_parts = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text_parts.append(page_text)\n",
    "        return \"\\n\\n\".join(text_parts).strip()\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error extracting PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def fixed_length_chunking(text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    idx = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append({\"text\": text[start:end], \"chunk_index\": idx, \"start_char\": start, \"end_char\": min(end, len(text)), \"strategy\": \"fixed\"})\n",
    "        start += chunk_size - overlap\n",
    "        idx += 1\n",
    "    return chunks\n",
    "\n",
    "def semantic_chunking(text: str, target_size: int = 500) -> List[Dict]:\n",
    "    if not text:\n",
    "        return []\n",
    "    if NLTK_AVAILABLE:\n",
    "        try:\n",
    "            sentences = sent_tokenize(text)\n",
    "        except:\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    else:\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    idx = 0\n",
    "    current_start = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if current_length + len(sentence) > target_size and current_chunk:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunks.append({\"text\": chunk_text, \"chunk_index\": idx, \"start_char\": current_start, \"end_char\": current_start + len(chunk_text), \"strategy\": \"semantic\"})\n",
    "            current_start += len(chunk_text) + 1\n",
    "            idx += 1\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += len(sentence) + 1\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunk_text = \" \".join(current_chunk)\n",
    "        chunks.append({\"text\": chunk_text, \"chunk_index\": idx, \"start_char\": current_start, \"end_char\": current_start + len(chunk_text), \"strategy\": \"semantic\"})\n",
    "    return chunks\n",
    "\n",
    "def recursive_chunking(text: str, max_chunk_size: int = 500) -> List[Dict]:\n",
    "    if not text or len(text) <= max_chunk_size:\n",
    "        return [{\"text\": text, \"chunk_index\": 0, \"start_char\": 0, \"end_char\": len(text), \"strategy\": \"recursive\"}] if text else []\n",
    "    \n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        if len(para) <= max_chunk_size:\n",
    "            chunks.append({\"text\": para, \"chunk_index\": len(chunks), \"start_char\": 0, \"end_char\": len(para), \"strategy\": \"recursive\"})\n",
    "        else:\n",
    "            sub_chunks = fixed_length_chunking(para, max_chunk_size, 50)\n",
    "            for sc in sub_chunks:\n",
    "                sc[\"chunk_index\"] = len(chunks)\n",
    "                sc[\"strategy\"] = \"recursive\"\n",
    "                chunks.append(sc)\n",
    "    return chunks\n",
    "\n",
    "def get_chunks(text: str, strategy: str, **kwargs) -> List[Dict]:\n",
    "    if strategy == \"fixed\":\n",
    "        return fixed_length_chunking(text, **kwargs)\n",
    "    elif strategy == \"semantic\":\n",
    "        return semantic_chunking(text, **kwargs)\n",
    "    elif strategy == \"recursive\":\n",
    "        return recursive_chunking(text, **kwargs)\n",
    "    return fixed_length_chunking(text, **kwargs)\n",
    "\n",
    "# ChromaDB\n",
    "_chroma_client = None\n",
    "\n",
    "def get_chroma_client():\n",
    "    global _chroma_client\n",
    "    if _chroma_client is None:\n",
    "        _chroma_client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "    return _chroma_client\n",
    "\n",
    "def get_collection_name(model: str, strategy: str) -> str:\n",
    "    return f\"{model.replace('/', '_').replace('-', '_')}_{strategy}\"\n",
    "\n",
    "def initialize_chromadb(collection_name: str):\n",
    "    client = get_chroma_client()\n",
    "    return client.get_or_create_collection(name=collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "def index_documents(chunks, embeddings, source_file, collection, embedding_model, chunking_strategy):\n",
    "    if not chunks or not embeddings:\n",
    "        return 0\n",
    "    ids = []\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
    "        chunk_id = f\"{source_file}_{chunking_strategy}_{i}\"\n",
    "        ids.append(chunk_id)\n",
    "        documents.append(chunk[\"text\"])\n",
    "        metadatas.append({\n",
    "            \"source_file\": source_file,\n",
    "            \"chunk_index\": chunk.get(\"chunk_index\", i),\n",
    "            \"chunking_strategy\": chunking_strategy,\n",
    "            \"embedding_model\": embedding_model,\n",
    "            \"indexed_at\": datetime.now().isoformat()\n",
    "        })\n",
    "    collection.add(ids=ids, documents=documents, embeddings=embeddings, metadatas=metadatas)\n",
    "    return len(ids)\n",
    "\n",
    "def retrieve_similar_chunks(query, collection, embedding_model, api_key, top_k=5):\n",
    "    query_embedding = get_embedding(query, embedding_model, api_key)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=top_k, include=[\"documents\", \"metadatas\", \"distances\"])\n",
    "    formatted = []\n",
    "    if results and results['ids'] and len(results['ids']) > 0:\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            formatted.append({\n",
    "                \"chunk_id\": results['ids'][0][i],\n",
    "                \"text\": results['documents'][0][i] if results['documents'] else \"\",\n",
    "                \"metadata\": results['metadatas'][0][i] if results['metadatas'] else {},\n",
    "                \"distance\": results['distances'][0][i] if results['distances'] else 0,\n",
    "                \"similarity\": 1 - results['distances'][0][i] if results['distances'] else 1\n",
    "            })\n",
    "    return formatted\n",
    "\n",
    "def list_collections():\n",
    "    client = get_chroma_client()\n",
    "    return [c.name for c in client.list_collections()]\n",
    "\n",
    "def delete_collection(collection_name):\n",
    "    try:\n",
    "        client = get_chroma_client()\n",
    "        client.delete_collection(collection_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_collection_stats(collection_name):\n",
    "    try:\n",
    "        client = get_chroma_client()\n",
    "        collection = client.get_collection(collection_name)\n",
    "        return {\"name\": collection_name, \"count\": collection.count()}\n",
    "    except Exception as e:\n",
    "        return {\"name\": collection_name, \"error\": str(e)}\n",
    "\n",
    "# ============================================================\n",
    "# Streamlit App\n",
    "# ============================================================\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"RAG Resume Analysis\",\n",
    "    page_icon=\"ðŸ“„\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Sidebar navigation\n",
    "st.sidebar.title(\"ðŸ“„ RAG Resume Analysis\")\n",
    "page = st.sidebar.radio(\"Navigation\", [\"âš™ï¸ Configuration & Upload\", \"ðŸ“Š Evaluation & Testing\"])\n",
    "\n",
    "# Session state initialization\n",
    "if \"api_key\" not in st.session_state:\n",
    "    st.session_state.api_key = \"\"\n",
    "if \"api_valid\" not in st.session_state:\n",
    "    st.session_state.api_valid = False\n",
    "if \"indexed_stats\" not in st.session_state:\n",
    "    st.session_state.indexed_stats = None\n",
    "\n",
    "# ============================================================\n",
    "# Page 1: Configuration & Upload\n",
    "# ============================================================\n",
    "if page == \"âš™ï¸ Configuration & Upload\":\n",
    "    st.title(\"âš™ï¸ Configuration & Upload\")\n",
    "    \n",
    "    # API Key Section\n",
    "    st.header(\"ðŸ”‘ API Configuration\")\n",
    "    col1, col2 = st.columns([3, 1])\n",
    "    with col1:\n",
    "        api_key = st.text_input(\"OpenRouter API Key\", type=\"password\", value=st.session_state.api_key)\n",
    "        st.session_state.api_key = api_key\n",
    "    with col2:\n",
    "        if st.button(\"Validate Key\"):\n",
    "            if api_key:\n",
    "                with st.spinner(\"Validating...\"):\n",
    "                    is_valid, message = validate_openrouter_key(api_key)\n",
    "                    st.session_state.api_valid = is_valid\n",
    "                    if is_valid:\n",
    "                        st.success(message)\n",
    "                    else:\n",
    "                        st.error(message)\n",
    "            else:\n",
    "                st.warning(\"Please enter an API key\")\n",
    "    \n",
    "    if st.session_state.api_valid:\n",
    "        st.success(\"âœ“ API Key validated\")\n",
    "    \n",
    "    st.divider()\n",
    "    \n",
    "    # Model & Strategy Selection\n",
    "    st.header(\"ðŸ¤– Model & Strategy Selection\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        selected_model = st.selectbox(\"Embedding Model\", EMBEDDING_MODELS)\n",
    "    with col2:\n",
    "        selected_strategy = st.selectbox(\"Chunking Strategy\", CHUNKING_STRATEGIES)\n",
    "    \n",
    "    # Chunking Parameters\n",
    "    with st.expander(\"Advanced Chunking Parameters\"):\n",
    "        chunk_size = st.slider(\"Chunk Size (characters)\", 100, 2000, DEFAULT_CHUNK_SIZE)\n",
    "        overlap = st.slider(\"Overlap (characters)\", 0, 200, DEFAULT_OVERLAP)\n",
    "    \n",
    "    st.divider()\n",
    "    \n",
    "    # File Upload Section\n",
    "    st.header(\"ðŸ“ Resume Upload\")\n",
    "    \n",
    "    # Option 1: Upload files\n",
    "    uploaded_files = st.file_uploader(\"Upload PDF Resumes\", type=[\"pdf\"], accept_multiple_files=True)\n",
    "    \n",
    "    # Option 2: Use files from data/ folder\n",
    "    existing_pdfs = list(DATA_DIR.glob(\"*.pdf\"))\n",
    "    if existing_pdfs:\n",
    "        st.info(f\"ðŸ“‚ Found {len(existing_pdfs)} PDF files in data/ folder\")\n",
    "        use_existing = st.checkbox(\"Use existing PDFs from data/ folder\", value=True)\n",
    "    else:\n",
    "        use_existing = False\n",
    "        st.info(\"ðŸ’¡ Add PDF files to the 'data/' folder or upload them above\")\n",
    "    \n",
    "    st.divider()\n",
    "    \n",
    "    # Process Button\n",
    "    st.header(\"ðŸš€ Process & Index\")\n",
    "    \n",
    "    if st.button(\"Process & Index Resumes\", type=\"primary\", disabled=not st.session_state.api_valid):\n",
    "        pdf_paths = []\n",
    "        \n",
    "        # Save uploaded files\n",
    "        if uploaded_files:\n",
    "            for uploaded_file in uploaded_files:\n",
    "                file_path = DATA_DIR / uploaded_file.name\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(uploaded_file.getbuffer())\n",
    "                pdf_paths.append(file_path)\n",
    "        \n",
    "        # Add existing PDFs\n",
    "        if use_existing and existing_pdfs:\n",
    "            pdf_paths.extend(existing_pdfs)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        pdf_paths = list(set(pdf_paths))\n",
    "        \n",
    "        if not pdf_paths:\n",
    "            st.error(\"No PDF files to process!\")\n",
    "        else:\n",
    "            collection_name = get_collection_name(selected_model, selected_strategy)\n",
    "            \n",
    "            progress_bar = st.progress(0)\n",
    "            status_text = st.empty()\n",
    "            \n",
    "            total_chunks = 0\n",
    "            processed = 0\n",
    "            \n",
    "            for i, pdf_path in enumerate(pdf_paths):\n",
    "                status_text.text(f\"Processing: {pdf_path.name}\")\n",
    "                \n",
    "                # Extract text\n",
    "                text = extract_text_from_pdf(str(pdf_path))\n",
    "                if not text:\n",
    "                    continue\n",
    "                \n",
    "                # Chunk\n",
    "                chunks = get_chunks(text, selected_strategy, chunk_size=chunk_size, overlap=overlap)\n",
    "                if not chunks:\n",
    "                    continue\n",
    "                \n",
    "                # Get embeddings\n",
    "                chunk_texts = [c[\"text\"] for c in chunks]\n",
    "                embeddings = []\n",
    "                for ct in chunk_texts:\n",
    "                    emb = get_embedding(ct, selected_model, st.session_state.api_key)\n",
    "                    embeddings.append(emb)\n",
    "                \n",
    "                # Index\n",
    "                collection = initialize_chromadb(collection_name)\n",
    "                indexed = index_documents(chunks, embeddings, pdf_path.name, collection, selected_model, selected_strategy)\n",
    "                \n",
    "                total_chunks += indexed\n",
    "                processed += 1\n",
    "                progress_bar.progress((i + 1) / len(pdf_paths))\n",
    "            \n",
    "            status_text.text(\"Complete!\")\n",
    "            \n",
    "            st.session_state.indexed_stats = {\n",
    "                \"collection_name\": collection_name,\n",
    "                \"files_processed\": processed,\n",
    "                \"total_chunks\": total_chunks,\n",
    "                \"model\": selected_model,\n",
    "                \"strategy\": selected_strategy\n",
    "            }\n",
    "            \n",
    "            st.success(f\"âœ… Indexed {total_chunks} chunks from {processed} files into collection: {collection_name}\")\n",
    "    \n",
    "    # Show indexed stats\n",
    "    if st.session_state.indexed_stats:\n",
    "        st.divider()\n",
    "        st.subheader(\"ðŸ“Š Last Indexing Summary\")\n",
    "        stats = st.session_state.indexed_stats\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        col1.metric(\"Files Processed\", stats[\"files_processed\"])\n",
    "        col2.metric(\"Total Chunks\", stats[\"total_chunks\"])\n",
    "        col3.metric(\"Collection\", stats[\"collection_name\"][:30] + \"...\")\n",
    "    \n",
    "    # Collection Management\n",
    "    st.divider()\n",
    "    st.header(\"ðŸ—„ï¸ Collection Management\")\n",
    "    \n",
    "    collections = list_collections()\n",
    "    if collections:\n",
    "        st.write(f\"**Existing Collections:** {len(collections)}\")\n",
    "        \n",
    "        for coll_name in collections:\n",
    "            stats = get_collection_stats(coll_name)\n",
    "            col1, col2, col3 = st.columns([3, 1, 1])\n",
    "            col1.write(f\"ðŸ“ {coll_name}\")\n",
    "            col2.write(f\"Chunks: {stats.get('count', 'N/A')}\")\n",
    "            if col3.button(\"Delete\", key=f\"del_{coll_name}\"):\n",
    "                if delete_collection(coll_name):\n",
    "                    st.success(f\"Deleted {coll_name}\")\n",
    "                    st.rerun()\n",
    "    else:\n",
    "        st.info(\"No collections found. Process some resumes first!\")\n",
    "\n",
    "# ============================================================\n",
    "# Page 2: Evaluation & Testing\n",
    "# ============================================================\n",
    "else:\n",
    "    st.title(\"ðŸ“Š Evaluation & Testing\")\n",
    "    \n",
    "    if not st.session_state.api_valid:\n",
    "        st.warning(\"âš ï¸ Please validate your API key on the Configuration page first!\")\n",
    "        st.stop()\n",
    "    \n",
    "    # Collection Selection\n",
    "    collections = list_collections()\n",
    "    if not collections:\n",
    "        st.warning(\"No collections found. Please index some resumes first!\")\n",
    "        st.stop()\n",
    "    \n",
    "    st.header(\"ðŸŽ¯ Query Configuration\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        selected_collection = st.selectbox(\"Select Collection\", collections)\n",
    "    with col2:\n",
    "        top_k = st.number_input(\"Top-K Results\", min_value=1, max_value=20, value=DEFAULT_TOP_K)\n",
    "    \n",
    "    # Parse collection name to get model\n",
    "    collection_parts = selected_collection.rsplit(\"_\", 1)\n",
    "    if len(collection_parts) == 2:\n",
    "        model_part = collection_parts[0].replace(\"_\", \"/\").replace(\"openai/\", \"openai/\")\n",
    "        # Find matching model\n",
    "        eval_model = None\n",
    "        for m in EMBEDDING_MODELS:\n",
    "            if m.replace(\"/\", \"_\").replace(\"-\", \"_\") in selected_collection:\n",
    "                eval_model = m\n",
    "                break\n",
    "        if not eval_model:\n",
    "            eval_model = EMBEDDING_MODELS[0]\n",
    "    else:\n",
    "        eval_model = EMBEDDING_MODELS[0]\n",
    "    \n",
    "    st.info(f\"Using model: {eval_model}\")\n",
    "    \n",
    "    st.divider()\n",
    "    \n",
    "    # Query Input\n",
    "    st.header(\"ðŸ” Evaluation Queries\")\n",
    "    \n",
    "    query_input = st.text_area(\n",
    "        \"Enter queries (one per line)\",\n",
    "        height=150,\n",
    "        placeholder=\"What is the candidate's email?\\nWhat programming languages does the candidate know?\\nWhat is their education background?\"\n",
    "    )\n",
    "    \n",
    "    # Ground Truth (optional)\n",
    "    with st.expander(\"ðŸ“‹ Ground Truth (Optional)\"):\n",
    "        st.write(\"Upload or paste ground truth JSON for accurate metrics calculation\")\n",
    "        gt_json = st.text_area(\"Ground Truth JSON\", height=100, placeholder='{\"q_0\": {\"relevant\": [\"answer1\", \"answer2\"]}}')\n",
    "        ground_truth = {}\n",
    "        if gt_json:\n",
    "            try:\n",
    "                ground_truth = json.loads(gt_json)\n",
    "            except:\n",
    "                st.error(\"Invalid JSON format\")\n",
    "    \n",
    "    st.divider()\n",
    "    \n",
    "    # Run Evaluation\n",
    "    if st.button(\"ðŸš€ Run Evaluation\", type=\"primary\"):\n",
    "        queries = [q.strip() for q in query_input.strip().split(\"\\n\") if q.strip()]\n",
    "        \n",
    "        if not queries:\n",
    "            st.error(\"Please enter at least one query\")\n",
    "        else:\n",
    "            collection = initialize_chromadb(selected_collection)\n",
    "            \n",
    "            results = []\n",
    "            all_metrics = []\n",
    "            \n",
    "            progress = st.progress(0)\n",
    "            \n",
    "            for i, query in enumerate(queries):\n",
    "                start_time = time.time()\n",
    "                retrieved = retrieve_similar_chunks(query, collection, eval_model, st.session_state.api_key, top_k)\n",
    "                latency = (time.time() - start_time) * 1000\n",
    "                \n",
    "                # Calculate metrics\n",
    "                retrieved_ids = [r[\"chunk_id\"] for r in retrieved]\n",
    "                relevant = ground_truth.get(f\"q_{i}\", {}).get(\"relevant\", [])\n",
    "                \n",
    "                if relevant:\n",
    "                    precision = len(set(retrieved_ids) & set(relevant)) / len(retrieved_ids) if retrieved_ids else 0\n",
    "                    recall = len(set(retrieved_ids) & set(relevant)) / len(relevant) if relevant else 0\n",
    "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                else:\n",
    "                    precision = recall = f1 = 0\n",
    "                \n",
    "                avg_sim = np.mean([r[\"similarity\"] for r in retrieved]) if retrieved else 0\n",
    "                \n",
    "                all_metrics.append({\n",
    "                    \"Query\": query[:50] + \"...\" if len(query) > 50 else query,\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall,\n",
    "                    \"F1\": f1,\n",
    "                    \"Avg Similarity\": avg_sim,\n",
    "                    \"Latency (ms)\": latency\n",
    "                })\n",
    "                \n",
    "                results.append({\"query\": query, \"results\": retrieved, \"metrics\": all_metrics[-1]})\n",
    "                progress.progress((i + 1) / len(queries))\n",
    "            \n",
    "            # Display Results\n",
    "            st.header(\"ðŸ“Š Results\")\n",
    "            \n",
    "            # Metrics Summary\n",
    "            metrics_df = pd.DataFrame(all_metrics)\n",
    "            \n",
    "            col1, col2, col3, col4 = st.columns(4)\n",
    "            col1.metric(\"Avg Precision\", f\"{metrics_df['Precision'].mean():.3f}\")\n",
    "            col2.metric(\"Avg Recall\", f\"{metrics_df['Recall'].mean():.3f}\")\n",
    "            col3.metric(\"Avg F1\", f\"{metrics_df['F1'].mean():.3f}\")\n",
    "            col4.metric(\"Avg Latency\", f\"{metrics_df['Latency (ms)'].mean():.1f} ms\")\n",
    "            \n",
    "            st.divider()\n",
    "            \n",
    "            # Metrics Table\n",
    "            st.subheader(\"ðŸ“‹ Per-Query Metrics\")\n",
    "            st.dataframe(metrics_df, use_container_width=True)\n",
    "            \n",
    "            # Similarity Chart\n",
    "            if len(metrics_df) > 0:\n",
    "                fig = px.bar(metrics_df, x=\"Query\", y=\"Avg Similarity\", title=\"Context Relevance by Query\")\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            st.divider()\n",
    "            \n",
    "            # Retrieved Chunks\n",
    "            st.subheader(\"ðŸ” Retrieved Chunks\")\n",
    "            \n",
    "            for r in results:\n",
    "                with st.expander(f\"Query: {r['query'][:80]}...\"):\n",
    "                    for j, chunk in enumerate(r[\"results\"]):\n",
    "                        st.markdown(f\"**Rank {j+1}** (Similarity: {chunk['similarity']:.3f})\")\n",
    "                        st.text(chunk[\"text\"][:500] + \"...\" if len(chunk[\"text\"]) > 500 else chunk[\"text\"])\n",
    "                        st.caption(f\"Source: {chunk['metadata'].get('source_file', 'Unknown')}\")\n",
    "                        st.divider()\n",
    "            \n",
    "            # Export\n",
    "            st.divider()\n",
    "            st.subheader(\"ðŸ’¾ Export Results\")\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                csv_data = metrics_df.to_csv(index=False)\n",
    "                st.download_button(\"ðŸ“¥ Download CSV\", csv_data, \"evaluation_results.csv\", \"text/csv\")\n",
    "            \n",
    "            with col2:\n",
    "                json_data = json.dumps({\"metrics\": all_metrics, \"results\": [{\"query\": r[\"query\"], \"retrieved_count\": len(r[\"results\"])} for r in results]}, indent=2)\n",
    "                st.download_button(\"ðŸ“¥ Download JSON\", json_data, \"evaluation_results.json\", \"application/json\")\n",
    "    \n",
    "    # A/B Comparison\n",
    "    st.divider()\n",
    "    st.header(\"ðŸ”¬ A/B Comparison\")\n",
    "    \n",
    "    if len(collections) > 1:\n",
    "        st.write(\"Compare multiple model+strategy configurations:\")\n",
    "        \n",
    "        selected_for_comparison = st.multiselect(\"Select collections to compare\", collections, default=collections[:2] if len(collections) >= 2 else collections)\n",
    "        \n",
    "        if st.button(\"Compare Selected\") and selected_for_comparison and query_input:\n",
    "            queries = [q.strip() for q in query_input.strip().split(\"\\n\") if q.strip()]\n",
    "            \n",
    "            comparison_results = []\n",
    "            \n",
    "            for coll_name in selected_for_comparison:\n",
    "                collection = initialize_chromadb(coll_name)\n",
    "                \n",
    "                # Find model for this collection\n",
    "                coll_model = eval_model\n",
    "                for m in EMBEDDING_MODELS:\n",
    "                    if m.replace(\"/\", \"_\").replace(\"-\", \"_\") in coll_name:\n",
    "                        coll_model = m\n",
    "                        break\n",
    "                \n",
    "                total_sim = 0\n",
    "                total_latency = 0\n",
    "                \n",
    "                for query in queries:\n",
    "                    start = time.time()\n",
    "                    retrieved = retrieve_similar_chunks(query, collection, coll_model, st.session_state.api_key, top_k)\n",
    "                    latency = (time.time() - start) * 1000\n",
    "                    \n",
    "                    total_sim += np.mean([r[\"similarity\"] for r in retrieved]) if retrieved else 0\n",
    "                    total_latency += latency\n",
    "                \n",
    "                comparison_results.append({\n",
    "                    \"Collection\": coll_name,\n",
    "                    \"Avg Similarity\": total_sim / len(queries),\n",
    "                    \"Avg Latency (ms)\": total_latency / len(queries)\n",
    "                })\n",
    "            \n",
    "            comparison_df = pd.DataFrame(comparison_results)\n",
    "            st.dataframe(comparison_df, use_container_width=True)\n",
    "            \n",
    "            # Chart\n",
    "            fig = px.bar(comparison_df, x=\"Collection\", y=\"Avg Similarity\", title=\"Comparison: Average Context Relevance\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    else:\n",
    "        st.info(\"Index resumes with different models/strategies to enable comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c7180",
   "metadata": {},
   "source": [
    "## 6. Sample Data Files\n",
    "\n",
    "Run the cells below to create sample metadata and ground truth files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a68e0bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created sample metadata file: data\\metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Create sample metadata.csv\n",
    "metadata_content = \"\"\"source_file,candidate_id,candidate_name,emails,phones,universities,education_timeline,skills,projects,notable_keywords\n",
    "resume1.pdf,cand_001,John Doe,john.doe@email.com,+1-555-0101,MIT,BS Computer Science|MIT|2018|2022,Python;JavaScript;React;Node.js;SQL,E-commerce Platform;ML Pipeline,machine learning;web development\n",
    "resume2.pdf,cand_002,Jane Smith,jane.smith@email.com;jsmith@work.com,+1-555-0102,Stanford University,MS Data Science|Stanford|2020|2022;BS Mathematics|UCLA|2016|2020,Python;R;TensorFlow;PyTorch;SQL;Tableau,Fraud Detection System;Customer Segmentation,data science;analytics\n",
    "resume3.pdf,cand_003,Alex Johnson,alex.j@email.com,,Harvard University,MBA|Harvard|2019|2021;BS Finance|NYU|2013|2017,Excel;PowerBI;SQL;Python;Financial Modeling,Investment Analysis Tool;Risk Assessment Dashboard,finance;business intelligence\n",
    "\"\"\"\n",
    "\n",
    "metadata_path = DATA_DIR / \"metadata.csv\"\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(metadata_content)\n",
    "\n",
    "print(f\"âœ… Created sample metadata file: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f2efd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created sample ground truth file: data\\ground_truth.json\n",
      "\n",
      "ðŸ“‹ Ground truth contains 5 sample queries\n"
     ]
    }
   ],
   "source": [
    "# Create sample ground_truth.json\n",
    "ground_truth_content = {\n",
    "    \"q_0\": {\n",
    "        \"query_text\": \"What is the candidate's email address?\",\n",
    "        \"relevant\": [\n",
    "            {\"source_file\": \"resume1.pdf\", \"candidate_id\": \"cand_001\", \"type\": \"exact_answer\", \"value\": \"john.doe@email.com\"},\n",
    "            {\"source_file\": \"resume2.pdf\", \"candidate_id\": \"cand_002\", \"type\": \"exact_answer\", \"value\": \"jane.smith@email.com\"}\n",
    "        ]\n",
    "    },\n",
    "    \"q_1\": {\n",
    "        \"query_text\": \"What programming languages does the candidate know?\",\n",
    "        \"relevant\": [\n",
    "            {\"source_file\": \"resume1.pdf\", \"candidate_id\": \"cand_001\", \"type\": \"partial_answer\", \"value\": \"Python, JavaScript\"},\n",
    "            {\"source_file\": \"resume2.pdf\", \"candidate_id\": \"cand_002\", \"type\": \"partial_answer\", \"value\": \"Python, R\"}\n",
    "        ]\n",
    "    },\n",
    "    \"q_2\": {\n",
    "        \"query_text\": \"What is the candidate's educational background?\",\n",
    "        \"relevant\": [\n",
    "            {\"source_file\": \"resume1.pdf\", \"candidate_id\": \"cand_001\", \"type\": \"partial_answer\", \"value\": \"BS Computer Science from MIT\"},\n",
    "            {\"source_file\": \"resume2.pdf\", \"candidate_id\": \"cand_002\", \"type\": \"partial_answer\", \"value\": \"MS Data Science from Stanford, BS Mathematics from UCLA\"},\n",
    "            {\"source_file\": \"resume3.pdf\", \"candidate_id\": \"cand_003\", \"type\": \"partial_answer\", \"value\": \"MBA from Harvard, BS Finance from NYU\"}\n",
    "        ]\n",
    "    },\n",
    "    \"q_3\": {\n",
    "        \"query_text\": \"What projects has the candidate worked on?\",\n",
    "        \"relevant\": [\n",
    "            {\"source_file\": \"resume1.pdf\", \"candidate_id\": \"cand_001\", \"type\": \"partial_answer\", \"value\": \"E-commerce Platform, ML Pipeline\"},\n",
    "            {\"source_file\": \"resume2.pdf\", \"candidate_id\": \"cand_002\", \"type\": \"partial_answer\", \"value\": \"Fraud Detection System, Customer Segmentation\"}\n",
    "        ]\n",
    "    },\n",
    "    \"q_4\": {\n",
    "        \"query_text\": \"Does the candidate have machine learning experience?\",\n",
    "        \"relevant\": [\n",
    "            {\"source_file\": \"resume1.pdf\", \"candidate_id\": \"cand_001\", \"type\": \"boolean_answer\", \"value\": \"yes\"},\n",
    "            {\"source_file\": \"resume2.pdf\", \"candidate_id\": \"cand_002\", \"type\": \"boolean_answer\", \"value\": \"yes\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "ground_truth_path = DATA_DIR / \"ground_truth.json\"\n",
    "with open(ground_truth_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(ground_truth_content, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Created sample ground truth file: {ground_truth_path}\")\n",
    "print(f\"\\nðŸ“‹ Ground truth contains {len(ground_truth_content)} sample queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dbcd0",
   "metadata": {},
   "source": [
    "## 7. Test Implementation\n",
    "\n",
    "Quick tests to verify all modules work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "328312e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Chunking Strategies\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“¦ Fixed Chunking (300 chars, 50 overlap):\n",
      "   Chunks created: 5\n",
      "   First chunk length: 300 chars\n",
      "\n",
      "ðŸ“¦ Semantic Chunking (target 300 chars):\n",
      "   Chunks created: 4\n",
      "   First chunk length: 280 chars\n",
      "\n",
      "ðŸ“¦ Recursive Chunking (max 300 chars):\n",
      "   Chunks created: 7\n",
      "   First chunk length: 26 chars\n",
      "\n",
      "============================================================\n",
      "âœ… All chunking strategies working correctly!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Test: Chunking Strategies\n",
    "# ============================================================\n",
    "\n",
    "sample_text = \"\"\"\n",
    "John Doe\n",
    "Software Engineer\n",
    "\n",
    "Contact Information:\n",
    "Email: john.doe@email.com\n",
    "Phone: +1-555-0101\n",
    "LinkedIn: linkedin.com/in/johndoe\n",
    "\n",
    "Summary:\n",
    "Experienced software engineer with 5+ years of expertise in full-stack development.\n",
    "Proficient in Python, JavaScript, and cloud technologies. Passionate about building\n",
    "scalable applications and mentoring junior developers.\n",
    "\n",
    "Education:\n",
    "Bachelor of Science in Computer Science\n",
    "Massachusetts Institute of Technology (MIT)\n",
    "Graduated: May 2022\n",
    "GPA: 3.8/4.0\n",
    "\n",
    "Experience:\n",
    "Senior Software Engineer - TechCorp Inc.\n",
    "January 2022 - Present\n",
    "- Led development of microservices architecture serving 1M+ users\n",
    "- Implemented CI/CD pipelines reducing deployment time by 60%\n",
    "- Mentored team of 5 junior developers\n",
    "\n",
    "Skills:\n",
    "Programming: Python, JavaScript, TypeScript, Java, SQL\n",
    "Frameworks: React, Node.js, Django, FastAPI\n",
    "Cloud: AWS, GCP, Docker, Kubernetes\n",
    "Tools: Git, Jenkins, Terraform\n",
    "\n",
    "Projects:\n",
    "1. E-commerce Platform - Built scalable shopping platform with React and Node.js\n",
    "2. ML Pipeline - Developed automated machine learning pipeline for predictions\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ§ª Testing Chunking Strategies\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test Fixed Chunking\n",
    "fixed_chunks = fixed_length_chunking(sample_text, chunk_size=300, overlap=50)\n",
    "print(f\"\\nðŸ“¦ Fixed Chunking (300 chars, 50 overlap):\")\n",
    "print(f\"   Chunks created: {len(fixed_chunks)}\")\n",
    "print(f\"   First chunk length: {len(fixed_chunks[0]['text'])} chars\")\n",
    "\n",
    "# Test Semantic Chunking\n",
    "semantic_chunks = semantic_chunking(sample_text, target_size=300)\n",
    "print(f\"\\nðŸ“¦ Semantic Chunking (target 300 chars):\")\n",
    "print(f\"   Chunks created: {len(semantic_chunks)}\")\n",
    "print(f\"   First chunk length: {len(semantic_chunks[0]['text'])} chars\")\n",
    "\n",
    "# Test Recursive Chunking\n",
    "recursive_chunks = recursive_chunking(sample_text, max_chunk_size=300)\n",
    "print(f\"\\nðŸ“¦ Recursive Chunking (max 300 chars):\")\n",
    "print(f\"   Chunks created: {len(recursive_chunks)}\")\n",
    "print(f\"   First chunk length: {len(recursive_chunks[0]['text'])} chars\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… All chunking strategies working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14cb9757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Evaluation Metrics\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Test Case:\n",
      "   Retrieved: ['chunk_1', 'chunk_2', 'chunk_3', 'chunk_4', 'chunk_5']\n",
      "   Relevant:  ['chunk_1', 'chunk_3', 'chunk_6', 'chunk_7']\n",
      "\n",
      "   Precision: 0.400 (expected: 0.400)\n",
      "   Recall:    0.500 (expected: 0.500)\n",
      "   F1 Score:  0.444 (expected: 0.444)\n",
      "\n",
      "   MRR: 0.500 (first hit at rank 2 and 2)\n",
      "\n",
      "============================================================\n",
      "âœ… All evaluation metrics working correctly!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Test: Evaluation Metrics\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ§ª Testing Evaluation Metrics\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test data\n",
    "retrieved = [\"chunk_1\", \"chunk_2\", \"chunk_3\", \"chunk_4\", \"chunk_5\"]\n",
    "relevant = [\"chunk_1\", \"chunk_3\", \"chunk_6\", \"chunk_7\"]\n",
    "\n",
    "precision = calculate_precision(retrieved, relevant)\n",
    "recall = calculate_recall(retrieved, relevant)\n",
    "f1 = calculate_f1_score(precision, recall)\n",
    "\n",
    "print(f\"\\nðŸ“Š Test Case:\")\n",
    "print(f\"   Retrieved: {retrieved}\")\n",
    "print(f\"   Relevant:  {relevant}\")\n",
    "print(f\"\\n   Precision: {precision:.3f} (expected: 0.400)\")\n",
    "print(f\"   Recall:    {recall:.3f} (expected: 0.500)\")\n",
    "print(f\"   F1 Score:  {f1:.3f} (expected: 0.444)\")\n",
    "\n",
    "# Test MRR\n",
    "ranked_results = [[\"chunk_5\", \"chunk_1\", \"chunk_3\"], [\"chunk_2\", \"chunk_6\"]]\n",
    "mrr_relevant = [\"chunk_1\", \"chunk_6\"]\n",
    "mrr = calculate_mrr(ranked_results, mrr_relevant)\n",
    "print(f\"\\n   MRR: {mrr:.3f} (first hit at rank 2 and 2)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… All evaluation metrics working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "238f30b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Ground Truth Functions\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“‹ Ground Truth loaded:\n",
      "   Number of queries: 5\n",
      "   Sample query: What is the candidate's email address?...\n",
      "\n",
      "ðŸ“‹ Metadata loaded:\n",
      "   Number of entries: 3\n",
      "   Columns: ['source_file', 'candidate_id', 'candidate_name', 'emails', 'phones', 'universities', 'education_timeline', 'skills', 'projects', 'notable_keywords']\n",
      "\n",
      "============================================================\n",
      "âœ… Ground truth functions working correctly!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Test: Ground Truth Loading\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ§ª Testing Ground Truth Functions\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the sample files we created\n",
    "gt = load_ground_truth()\n",
    "metadata = load_metadata()\n",
    "\n",
    "print(f\"\\nðŸ“‹ Ground Truth loaded:\")\n",
    "print(f\"   Number of queries: {len(gt)}\")\n",
    "if gt:\n",
    "    first_key = list(gt.keys())[0]\n",
    "    print(f\"   Sample query: {gt[first_key].get('query_text', 'N/A')[:50]}...\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Metadata loaded:\")\n",
    "print(f\"   Number of entries: {len(metadata)}\")\n",
    "if not metadata.empty:\n",
    "    print(f\"   Columns: {list(metadata.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Ground truth functions working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba23e95",
   "metadata": {},
   "source": [
    "## 8. Quick Start Guide\n",
    "\n",
    "### To use this RAG Resume Analysis system:\n",
    "\n",
    "1. **Add your OpenRouter API key** (get one at https://openrouter.ai)\n",
    "\n",
    "2. **Add resume PDFs** to the `data/` folder\n",
    "\n",
    "3. **Run all cells** in this notebook to set up the functions\n",
    "\n",
    "4. **Launch Streamlit app**:\n",
    "   ```bash\n",
    "   streamlit run app.py\n",
    "   ```\n",
    "\n",
    "5. **In the Streamlit UI**:\n",
    "   - Page 1: Enter API key, select model/strategy, upload or use existing PDFs, click \"Process & Index\"\n",
    "   - Page 2: Enter queries, run evaluation, view metrics, export results\n",
    "\n",
    "### Alternative: Use notebook directly\n",
    "\n",
    "You can also use the functions directly in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734b98ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Uncomment the code above and add your API key to test the full pipeline!\n",
      "\n",
      "ðŸ“± Or run: streamlit run app.py\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Example: Using the notebook directly\n",
    "# ============================================================\n",
    "\n",
    "# Uncomment and fill in your API key to test:\n",
    "# API_KEY = \"your-openrouter-api-key-here\"\n",
    "\n",
    "# Example workflow:\n",
    "\"\"\"\n",
    "# 1. Validate API key\n",
    "is_valid, msg = validate_openrouter_key(API_KEY)\n",
    "print(msg)\n",
    "\n",
    "# 2. Get PDF files\n",
    "pdf_files = get_pdf_files(DATA_DIR)\n",
    "\n",
    "# 3. Process and index resumes\n",
    "stats = process_resumes(\n",
    "    pdf_files=pdf_files,\n",
    "    chunking_strategy=\"semantic\",\n",
    "    embedding_model=\"openai/text-embedding-3-small\",\n",
    "    api_key=API_KEY\n",
    ")\n",
    "print(stats)\n",
    "\n",
    "# 4. Run evaluation queries\n",
    "queries = [\n",
    "    \"What is the candidate's email address?\",\n",
    "    \"What programming languages does the candidate know?\",\n",
    "    \"What is their education background?\"\n",
    "]\n",
    "\n",
    "results = evaluate_retrieval(\n",
    "    queries=queries,\n",
    "    collection_name=stats[\"collection_name\"],\n",
    "    embedding_model=\"openai/text-embedding-3-small\",\n",
    "    api_key=API_KEY,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# 5. Export results\n",
    "export_results_csv(results)\n",
    "export_results_json(results)\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ’¡ Uncomment the code above and add your API key to test the full pipeline!\")\n",
    "print(\"\\nðŸ“± Or run: streamlit run app.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
