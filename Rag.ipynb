{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cf5fe3",
   "metadata": {},
   "source": [
    "# üìÑ PDF RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\env1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import fitz  # PyMuPDF\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import requests\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f287ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ PDF Directory: pdfs\n",
      "üíæ ChromaDB Path: ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration Settings\n",
    "\"\"\"\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API_KEY not found.\") \n",
    "PDF_DIR = \"pdfs\"\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "\n",
    "# Text Processing Parameters\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 100\n",
    "TOP_K_RESULTS = 3\n",
    "\n",
    "# LLM Parameters\n",
    "MODEL_NAME = \"anthropic/claude-3.5-sonnet\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 600\n",
    "\n",
    "# Embedding Parameters\n",
    "EMBEDDING_MODEL = \"openai/text-embedding-3-small\"\n",
    "\n",
    "# Create necessary directories\n",
    "pathlib.Path(PDF_DIR).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(CHROMA_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ PDF Directory: {PDF_DIR}\")\n",
    "print(f\"üíæ ChromaDB Path: {CHROMA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d4554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ embedding function defined\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(texts: List[str], api_key: str, model_name: str = EMBEDDING_MODEL) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for input texts using OpenRouter API.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        api_key: OpenRouter API key\n",
    "        model_name: Model identifier on OpenRouter\n",
    "        \n",
    "    Returns:\n",
    "        List of embedding vectors\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/embeddings\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"HTTP-Referer\": \"http://localhost\",\n",
    "                \"X-Title\": \"Resume-RAG-System\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": model_name,\n",
    "                \"input\": texts\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            embeddings = [item['embedding'] for item in result.get('data', [])]\n",
    "            \n",
    "            if not embeddings:\n",
    "                raise ValueError(\"No embeddings returned from API\")\n",
    "            \n",
    "            return embeddings\n",
    "        else:\n",
    "            error_msg = f\"API Error {response.status_code}: {response.text}\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            raise ValueError(error_msg)\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        raise ValueError(\"Request timed out while generating embeddings\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Embedding generation failed: {str(e)}\")\n",
    "\n",
    "\n",
    "# Create embedding function wrapper for ChromaDB\n",
    "def create_embedding_function(api_key: str, model_name: str = EMBEDDING_MODEL):\n",
    "    \"\"\"Create a ChromaDB-compatible embedding function.\"\"\"\n",
    "    \n",
    "    class OpenRouterEmbeddings(embedding_functions.EmbeddingFunction):\n",
    "        def __call__(self, input: List[str]) -> List[List[float]]:\n",
    "            return generate_embeddings(input, api_key, model_name)\n",
    "    \n",
    "    return OpenRouterEmbeddings()\n",
    "\n",
    "\n",
    "print(\"‚úÖ embedding function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8099ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text processing functions defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PDF Processing Utilities\n",
    "\"\"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\", sort=True) + \"\\n\"\n",
    "        doc.close()\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize extracted text.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text from PDF\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    # Remove excessive newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Remove excessive spaces\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    \n",
    "    # Remove control characters\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f]', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, \n",
    "               chunk_overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks for better retrieval.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to split\n",
    "        chunk_size: Maximum chunk size\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Text processing functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6660adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM function defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LLM Invocation Function\n",
    "\"\"\"\n",
    "\n",
    "def invoke_llm(prompt: str, model_name: str = MODEL_NAME, \n",
    "               temperature: float = TEMPERATURE, \n",
    "               max_tokens: int = MAX_TOKENS) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the LLM using OpenRouter API.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt string\n",
    "        model_name: Model identifier on OpenRouter\n",
    "        temperature: Sampling temperature (0.0 = deterministic)\n",
    "        max_tokens: Maximum tokens in response\n",
    "        \n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {OS_API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"HTTP-Referer\": \"http://localhost\",\n",
    "                \"X-Title\": \"Resume-RAG-System\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": model_name,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        else:\n",
    "            return f\"API Error: {response.status_code} - {response.text}\"\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Error: Request timed out\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ LLM function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1fd184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retriever function defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ChromaDB Retrieval Function\n",
    "\"\"\"\n",
    "\n",
    "def retrieve_documents(query: str, collection, k: int = TOP_K_RESULTS) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for a query from ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        collection: ChromaDB collection instance\n",
    "        k: Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    if results['documents'] and results['documents'][0]:\n",
    "        for i, doc_text in enumerate(results['documents'][0]):\n",
    "            metadata = results['metadatas'][0][i] if results['metadatas'] else {}\n",
    "            documents.append(\n",
    "                Document(page_content=doc_text, metadata=metadata)\n",
    "            )\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "print(\"‚úÖ Retriever function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3b2080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt template created\n",
      "\n",
      "Template variables: ['context', 'question']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RAG Prompt Template\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a professional Resume Screening Assistant specialized in analyzing candidate qualifications.\n",
    "\n",
    "Context from Resume(s):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer using ONLY the information provided in the context above\n",
    "- Be specific and cite relevant details from the resume\n",
    "- If the requested information is not available in the context, explicitly state: \"This information is not available in the resume\"\n",
    "- Do not make assumptions or add information not present in the context\n",
    "- Keep your answer concise and focused on the question\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt template created\")\n",
    "print(f\"\\nTemplate variables: {prompt_template.input_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4382d263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document formatter defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper function to format retrieved documents\n",
    "\"\"\"\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Format retrieved documents into a context string.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of LangChain Document objects\n",
    "        \n",
    "    Returns:\n",
    "        Formatted context string\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return \"No relevant information found in the resumes.\"\n",
    "    \n",
    "    formatted_chunks = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('doc_name', 'Unknown')\n",
    "        formatted_chunks.append(\n",
    "            f\"[Chunk {i} from {source}]:\\n{doc.page_content}\"\n",
    "        )\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_chunks)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Document formatter defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f680804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain builder defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete RAG Pipeline using LangChain LCEL\n",
    "\"\"\"\n",
    "\n",
    "def build_rag_chain(collection, prompt: PromptTemplate, k: int = TOP_K_RESULTS):\n",
    "    \"\"\"\n",
    "    Construct a complete RAG chain using LangChain Expression Language.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create retriever function\n",
    "    def retriever_fn(query: str) -> List[Document]:\n",
    "        return retrieve_documents(query, collection, k)\n",
    "    \n",
    "    # Convert PromptValue to string before passing to LLM\n",
    "    def prompt_to_string(prompt_value):\n",
    "        \"\"\"Convert LangChain PromptValue to plain string.\"\"\"\n",
    "        return prompt_value.to_string()\n",
    "    \n",
    "    # Wrap functions in RunnableLambda to make them LCEL-compatible\n",
    "    retriever_runnable = RunnableLambda(retriever_fn)\n",
    "    format_docs_runnable = RunnableLambda(format_docs)\n",
    "    prompt_string_runnable = RunnableLambda(prompt_to_string)\n",
    "    llm_runnable = RunnableLambda(invoke_llm)\n",
    "    \n",
    "    # Build the chain using LCEL syntax\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever_runnable | format_docs_runnable,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | prompt_string_runnable\n",
    "        | llm_runnable\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "print(\"‚úÖ RAG chain builder defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae9d9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom embedding function and ChromaDB client initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_5664\\2060399078.py:56: DeprecationWarning: The class OpenRouterEmbeddings does not implement __init__. This will be required in a future version.\n",
      "  return OpenRouterEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup ChromaDB with Custom OpenRouter Embeddings\n",
    "\"\"\"\n",
    "\n",
    "# Initialize custom embedding function\n",
    "embedding_function = create_embedding_function(\n",
    "    api_key=OS_API_KEY,\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "print(\"‚úÖ Custom embedding function and ChromaDB client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45170a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Deleted existing collection\n",
      "\n",
      "üìÑ Indexing 12 PDF(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:14<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully indexed 80 chunks from 12 PDF(s)\n",
      "üìä Collection size: 80 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and Index Resume PDFs\n",
    "\"\"\"\n",
    "\n",
    "# Check for PDF files\n",
    "pdf_files = [f for f in os.listdir(PDF_DIR) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"‚ùå No PDFs found in {PDF_DIR}!\")\n",
    "    print(f\"Please add resume PDFs to the '{PDF_DIR}' directory.\")\n",
    "else:\n",
    "    # Reset collection\n",
    "    try:\n",
    "        client.delete_collection(\"resume_index\")\n",
    "        print(\"üóëÔ∏è  Deleted existing collection\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create new collection with custom embedding function\n",
    "    collection = client.create_collection(\n",
    "        name=\"resume_index\", \n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    # Index PDFs with batch processing\n",
    "    print(f\"\\nüìÑ Indexing {len(pdf_files)} PDF(s)...\")\n",
    "    \n",
    "    total_chunks = 0\n",
    "    batch_size = 10  # Process chunks in batches to avoid API limits\n",
    "    \n",
    "    for pdf_name in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        pdf_path = os.path.join(PDF_DIR, pdf_name)\n",
    "        \n",
    "        # Extract and process text\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "        if not raw_text:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {pdf_name} - no text extracted\")\n",
    "            continue\n",
    "            \n",
    "        cleaned_text = preprocess_text(raw_text)\n",
    "        chunks = chunk_text(cleaned_text)\n",
    "        \n",
    "        if chunks:\n",
    "            # Add chunks in batches\n",
    "            for i in range(0, len(chunks), batch_size):\n",
    "                batch_chunks = chunks[i:i + batch_size]\n",
    "                batch_ids = [f\"{pdf_name}_chunk_{j}\" for j in range(i, i + len(batch_chunks))]\n",
    "                batch_metadata = [{\"doc_name\": pdf_name} for _ in batch_chunks]\n",
    "                \n",
    "                try:\n",
    "                    collection.add(\n",
    "                        documents=batch_chunks,\n",
    "                        metadatas=batch_metadata,\n",
    "                        ids=batch_ids\n",
    "                    )\n",
    "                    total_chunks += len(batch_chunks)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n‚ùå Error adding batch from {pdf_name}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully indexed {total_chunks} chunks from {len(pdf_files)} PDF(s)\")\n",
    "    print(f\"üìä Collection size: {collection.count()} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eafa4c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain assembled successfully!\n",
      "\n",
      "üîó Chain Components:\n",
      "  1. Retriever: ChromaDB vector search\n",
      "  2. Formatter: Document context builder\n",
      "  3. Prompt: Resume screening template\n",
      "  4. LLM: Claude 3.5 Sonnet via OpenRouter\n",
      "  5. Parser: String output parser\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Assemble All Components into RAG Chain\n",
    "\"\"\"\n",
    "\n",
    "# Build the chain\n",
    "rag_chain = build_rag_chain(collection, prompt_template, k=TOP_K_RESULTS)\n",
    "\n",
    "print(\"‚úÖ RAG chain assembled successfully!\")\n",
    "print(\"\\nüîó Chain Components:\")\n",
    "print(\"  1. Retriever: ChromaDB vector search\")\n",
    "print(\"  2. Formatter: Document context builder\")\n",
    "print(\"  3. Prompt: Resume screening template\")\n",
    "print(\"  4. LLM: Claude 3.5 Sonnet via OpenRouter\")\n",
    "print(\"  5. Parser: String output parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50aa25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Interactive Resume Query System\n",
    "\"\"\"\n",
    "\n",
    "def run_interactive_qa():\n",
    "    \"\"\"\n",
    "    Run an interactive question-answering session.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"INTERACTIVE RESUME SCREENING SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nAsk questions about the resumes (type 'exit', 'quit', or 'q' to stop)\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"‚ùì Your question: \").strip()\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        if question.lower() in ['exit', 'quit', 'q']:\n",
    "            print(\"\\nüëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            print(\"\\nüîÑ Processing...\\n\")\n",
    "            answer = rag_chain.invoke(question)\n",
    "            print(f\"üí° Answer:\\n{answer}\\n\")\n",
    "            print(\"-\" * 80 + \"\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4504f1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Interactive Q&A function ready\n",
      "\n",
      "================================================================================\n",
      "INTERACTIVE RESUME SCREENING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Ask questions about the resumes (type 'exit', 'quit', or 'q' to stop)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üîÑ Processing...\n",
      "\n",
      "üí° Answer:\n",
      "According to the resume context, Bissan's email is: salehbissan22@gmail.com\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Interactive Q&A function ready\")\n",
    "run_interactive_qa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
